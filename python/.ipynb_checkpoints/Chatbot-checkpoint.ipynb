{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb226210",
   "metadata": {},
   "source": [
    "# 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f94a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통코드\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b1f64",
   "metadata": {},
   "source": [
    "## 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7a3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n",
    "corpus = pd.read_csv('./ChatbotData.csv')\n",
    "\n",
    "# 2,000개 데이터 셋만 활용 (Google Colab 일 경우 3,000개에서는 메모리 오버되는 현상 발생)\n",
    "texts = []\n",
    "pairs = []\n",
    "for i, (text, pair) in enumerate(zip(corpus['Q'], corpus['A'])):\n",
    "    texts.append(text)\n",
    "    pairs.append(pair)\n",
    "    \n",
    "    #메모리가 부족하면 데이터 개수 조절\n",
    "    if i >= 3000: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d33721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12시 땡!', '하루가 또 가네요.'),\n",
       " ('1지망 학교 떨어졌어', '위로해 드립니다.'),\n",
       " ('3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
       " ('3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
       " ('PPL 심하네', '눈살이 찌푸려지죠.'),\n",
       " ('SD카드 망가졌어', '다시 새로 사는 게 마음 편해요.'),\n",
       " ('SD카드 안돼', '다시 새로 사는 게 마음 편해요.'),\n",
       " ('SNS 맞팔 왜 안하지ㅠㅠ', '잘 모르고 있을 수도 있어요.'),\n",
       " ('SNS 시간낭비인 거 아는데 매일 하는 중', '시간을 정하고 해보세요.'),\n",
       " ('SNS 시간낭비인데 자꾸 보게됨', '시간을 정하고 해보세요.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question와 answer 데이터 확인\n",
    "list(zip(texts, pairs))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653510ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a8af66",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d123693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요\n",
      "TensorFlow\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_sentence(sentence):\n",
    "#     # 한글, 숫자, 영문 대/소문자를 제외한 모든 문자는 제거합니다.\n",
    "#     sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣ㅣa-zA-Z ]',r'', sentence)\n",
    "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
    "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣]',r'', sentence)\n",
    "    return sentence\n",
    "\n",
    "# 전처리 함수 테스트\n",
    "print(clean_sentence('안녕하세요~:)'))\n",
    "print(clean_sentence('TensorFlow^@^%#@!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08099864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 형태소 분석\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "def process_morph(sentence):\n",
    "    return ' '.join(okt.morphs(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56562a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#챗봇을 위해 필요한 데이터 생성 함수\n",
    "def clean_and_morph(sentence, is_question=True):\n",
    "    # 한글 문장 전처리\n",
    "    sentence = clean_sentence(sentence)\n",
    "    # 형태소 변환\n",
    "    sentence = process_morph(sentence)\n",
    "    # Question 인 경우, Answer인 경우를 분기하여 처리\n",
    "    # Answer에는 시작 과 종료 기호 추가\n",
    "    if is_question:\n",
    "        return sentence\n",
    "    else:\n",
    "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
    "        return ('<START> ' + sentence, sentence + ' <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552cfe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#챗봇을 위한 데이터 생성\n",
    "def preprocess(texts, pairs):\n",
    "    questions = []\n",
    "    answer_in = []\n",
    "    answer_out = []\n",
    "\n",
    "    # 질의에 대한 전처리\n",
    "    for text in texts:\n",
    "        # 전처리와 morph 수행\n",
    "        question = clean_and_morph(text, is_question=True)\n",
    "        questions.append(question)\n",
    "\n",
    "    # 답변에 대한 전처리\n",
    "    for pair in pairs:\n",
    "        # 전처리와 morph 수행\n",
    "        in_, out_ = clean_and_morph(pair, is_question=False)\n",
    "        answer_in.append(in_)\n",
    "        answer_out.append(out_)\n",
    "    \n",
    "    return questions, answer_in, answer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92b62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['맛있는 칵테일 추천 해줘', '20 대 여자 가 좋아할 만 한 칵테일 은']\n",
      "['<START> 어떤 맛 좋아하세요', '<START> 저 는 롱티 좋아해요']\n",
      "['어떤 맛 좋아하세요 <END>', '저 는 롱티 좋아해요 <END>']\n"
     ]
    }
   ],
   "source": [
    "#챗봇 훈련에 필요한 데이터 생성 및 확인\n",
    "questions, answer_in, answer_out = preprocess(texts, pairs)\n",
    "\n",
    "print(questions[:2])\n",
    "print(answer_in[:2])\n",
    "print(answer_out[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed053df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문장을 하나의 문장으로 생성\n",
    "all_sentences = questions + answer_in + answer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b23777cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 와 수치화 및 패딩\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# WARNING 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "# 텍스트를 시퀀스로 인코딩 (texts_to_sequences)\n",
    "question_sequence = tokenizer.texts_to_sequences(questions)\n",
    "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
    "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)\n",
    "\n",
    "# 문장의 길이 맞추기 (pad_sequences)\n",
    "MAX_LENGTH = 30\n",
    "question_padded = pad_sequences(question_sequence, \n",
    "                                maxlen=MAX_LENGTH, \n",
    "                                truncating='post', \n",
    "                                padding='post')\n",
    "answer_in_padded = pad_sequences(answer_in_sequence, \n",
    "                                 maxlen=MAX_LENGTH, \n",
    "                                 truncating='post', \n",
    "                                 padding='post')\n",
    "answer_out_padded = pad_sequences(answer_out_sequence, \n",
    "                                  maxlen=MAX_LENGTH, \n",
    "                                  truncating='post', \n",
    "                                  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc98333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV>\t -> \t1\n",
      "<START>\t -> \t2\n",
      "<END>\t -> \t3\n",
      "이\t -> \t4\n",
      "거\t -> \t5\n",
      "을\t -> \t6\n",
      "가\t -> \t7\n",
      "예요\t -> \t8\n",
      "도\t -> \t9\n",
      "해보세요\t -> \t10\n",
      "에\t -> \t11\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    print(f'{word}\\t -> \\t{idx}')\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06f5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4720"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 개수 확인\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945a057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3001, 30), (3001, 30), (3001, 30))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#시퀀스 확인\n",
    "question_padded.shape, answer_in_padded.shape, answer_out_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "873736d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 4721), (30, 4721))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원핫 인코딩\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "\n",
    "#원핫 인코딩을 위한 함수\n",
    "def convert_to_one_hot(padded):\n",
    "    # 원핫인코딩 초기화\n",
    "    one_hot_vector = np.zeros((len(answer_out_padded), \n",
    "                               MAX_LENGTH, \n",
    "                               VOCAB_SIZE))\n",
    "\n",
    "    # 디코더 목표를 원핫 인코딩으로 변환\n",
    "    # 학습시 입력은 인덱스이지만 출력은 원핫 인코딩 형식임\n",
    "    for i, sequence in enumerate(answer_out_padded):\n",
    "        for j, index in enumerate(sequence):\n",
    "            one_hot_vector[i, j, index] = 1\n",
    "\n",
    "    return one_hot_vector\n",
    "\n",
    "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
    "answer_out_one_hot = convert_to_one_hot(answer_out_padded)\n",
    "answer_in_one_hot[0].shape, answer_in_one_hot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3750db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 index를 다시 단어로 변환\n",
    "def convert_index_to_text(indexs, end_token): \n",
    "    \n",
    "    sentence = ''\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == end_token:\n",
    "            # 끝 단어이므로 예측 중비\n",
    "            break;\n",
    "        # 사전에 존재하는 단어의 경우 단어 추가\n",
    "        if index > 0 and tokenizer.index_word[index] is not None:\n",
    "            sentence += tokenizer.index_word[index]\n",
    "        else:\n",
    "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
    "            sentence += ''\n",
    "            \n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45603a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bde5f5a0",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b29d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff0f87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력을 위한 클래스 - Attention 사용\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, \n",
    "                                   embedding_dim, \n",
    "                                   input_length=time_steps, \n",
    "                                   name='Embedding')\n",
    "        self.dropout = Dropout(0.2, name='Dropout')\n",
    "        # (attention) return_sequences=True 추가\n",
    "        self.lstm = LSTM(units, \n",
    "                         return_state=True, \n",
    "                         return_sequences=True, \n",
    "                         name='LSTM')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x)\n",
    "        # (attention) x return 추가\n",
    "        return x, [hidden_state, cell_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b76ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력을 위한 디코더\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, \n",
    "                                   embedding_dim, \n",
    "                                   input_length=time_steps, \n",
    "                                   name='Embedding')\n",
    "        self.dropout = Dropout(0.2, name='Dropout')\n",
    "        self.lstm = LSTM(units, \n",
    "                         return_state=True, \n",
    "                         return_sequences=True, \n",
    "                         name='LSTM'\n",
    "                        )\n",
    "        self.attention = Attention(name='Attention')\n",
    "        self.dense = Dense(vocab_size, \n",
    "                           activation='softmax', \n",
    "                           name='Dense')\n",
    "    \n",
    "    def call(self, inputs, initial_state):\n",
    "        # (attention) encoder_inputs 추가\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        x = self.embedding(decoder_inputs)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
    "        \n",
    "        # (attention) key_value, attention_matrix 추가\n",
    "        # 이전 hidden_state의 값을 concat으로 만들어 vector를 생성합니다.        \n",
    "        key_value = tf.concat([initial_state[0][:, tf.newaxis, :], \n",
    "                               x[:, :-1, :]], axis=1)        \n",
    "        # 이전 hidden_state의 값을 concat으로 만든 vector와 encoder에서 나온 \n",
    "        # 출력 값들로 attention을 구합니다.\n",
    "        attention_matrix = self.attention([key_value, encoder_inputs])\n",
    "        # 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n",
    "        x = tf.concat([x, attention_matrix], axis=-1)\n",
    "        \n",
    "        x = self.dense(x)\n",
    "        return x, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e20a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
    "        super(ChatModel, self).__init__()\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
    "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        if training:\n",
    "            encoder_inputs, decoder_inputs = inputs\n",
    "            # (attention) encoder 출력 값 수정\n",
    "            encoder_outputs, context_vector = self.encoder(encoder_inputs)\n",
    "            # (attention) decoder 입력 값 수정\n",
    "            decoder_outputs, _, _ = self.decoder((encoder_outputs, decoder_inputs), \n",
    "                                                 initial_state=context_vector)\n",
    "            return decoder_outputs\n",
    "        else:\n",
    "            x = inputs\n",
    "            # (attention) encoder 출력 값 수정\n",
    "            encoder_outputs, context_vector = self.encoder(x)\n",
    "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
    "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
    "            \n",
    "            for i in tf.range(self.time_steps):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder((encoder_outputs, target_seq), \n",
    "                                                                            initial_state=context_vector)\n",
    "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
    "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
    "                results = results.write(i, decoder_output)\n",
    "                \n",
    "                if decoder_output == self.end_token:\n",
    "                    break\n",
    "                    \n",
    "                target_seq = decoder_output\n",
    "                context_vector = [decoder_hidden, decoder_cell]\n",
    "                \n",
    "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "456ef268",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4273ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 100\n",
    "TIME_STEPS = MAX_LENGTH\n",
    "START_TOKEN = tokenizer.word_index['<START>']\n",
    "END_TOKEN = tokenizer.word_index['<END>']\n",
    "\n",
    "UNITS = 128\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "DATA_LENGTH = len(questions)\n",
    "SAMPLE_SIZE = 3\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d62cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크 포인트 생성\n",
    "checkpoint_path = 'model/chatmodel-attention-checkpoint.ckpt'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True, \n",
    "                             monitor='loss', \n",
    "                             verbose=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03681dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 07:43:59.714200: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#모델 생성\n",
    "chat = ChatModel(UNITS, \n",
    "                  VOCAB_SIZE, \n",
    "                  EMBEDDING_DIM, \n",
    "                  TIME_STEPS, \n",
    "                  START_TOKEN, \n",
    "                  END_TOKEN)\n",
    "\n",
    "chat.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c6f809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 함수\n",
    "def make_prediction(model, question_inputs):\n",
    "    results = model(inputs=question_inputs, training=False)\n",
    "    # 변환된 인덱스를 문장으로 변환\n",
    "    results = np.asarray(results).reshape(-1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ec115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c943d37",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "814bed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing epoch: 1...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 07:44:05.514142: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"110\" frequency: 2400 num_cores: 8 environment { key: \"cpu_instruction_set\" value: \"SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 6291456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/188 [============================>.] - ETA: 0s - loss: 1.8972 - acc: 0.8046\n",
      "Epoch 1: loss improved from inf to 1.89479, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 69ms/step - loss: 1.8948 - acc: 0.8046\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 1.1280 - acc: 0.8383\n",
      "Epoch 2: loss improved from 1.89479 to 1.12936, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 1.1294 - acc: 0.8381\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 1.0533 - acc: 0.8423\n",
      "Epoch 3: loss improved from 1.12936 to 1.05330, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 1.0533 - acc: 0.8423\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 1.0088 - acc: 0.8454\n",
      "Epoch 4: loss improved from 1.05330 to 1.01129, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 1.0113 - acc: 0.8451\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9791 - acc: 0.8477\n",
      "Epoch 5: loss improved from 1.01129 to 0.97911, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.9791 - acc: 0.8477\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.9492 - acc: 0.8504\n",
      "Epoch 6: loss improved from 0.97911 to 0.94919, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.9492 - acc: 0.8504\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.9198 - acc: 0.8529\n",
      "Epoch 7: loss improved from 0.94919 to 0.91999, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.9200 - acc: 0.8528\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.8918 - acc: 0.8552\n",
      "Epoch 8: loss improved from 0.91999 to 0.89172, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.8917 - acc: 0.8552\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.8611 - acc: 0.8576\n",
      "Epoch 9: loss improved from 0.89172 to 0.86082, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.8608 - acc: 0.8576\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.8276 - acc: 0.8607\n",
      "Epoch 10: loss improved from 0.86082 to 0.82774, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.8277 - acc: 0.8607\n",
      "Q: 나 만 우스워질 거 같아\n",
      "A: 저 도 잘 해보세요 \n",
      "\n",
      "\n",
      "Q: 너무 멋있다\n",
      "A: 저 도 잘 해보세요 \n",
      "\n",
      "\n",
      "Q: 다리 가 퉁퉁 부었어\n",
      "A: 저 도 잘 해보세요 \n",
      "\n",
      "\n",
      "processing epoch: 11...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.7949 - acc: 0.8641\n",
      "Epoch 1: loss improved from 0.82774 to 0.79437, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.7944 - acc: 0.8642\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.7614 - acc: 0.8681\n",
      "Epoch 2: loss improved from 0.79437 to 0.76151, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.7615 - acc: 0.8680\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7284 - acc: 0.8718\n",
      "Epoch 3: loss improved from 0.76151 to 0.72840, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.7284 - acc: 0.8718\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6962 - acc: 0.8758\n",
      "Epoch 4: loss improved from 0.72840 to 0.69622, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.6962 - acc: 0.8758\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.8807\n",
      "Epoch 5: loss improved from 0.69622 to 0.66265, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.6627 - acc: 0.8806\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6285 - acc: 0.8857\n",
      "Epoch 6: loss improved from 0.66265 to 0.62850, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.6285 - acc: 0.8857\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.5952 - acc: 0.8907\n",
      "Epoch 7: loss improved from 0.62850 to 0.59520, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 72ms/step - loss: 0.5952 - acc: 0.8907\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.8965\n",
      "Epoch 8: loss improved from 0.59520 to 0.56262, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.5626 - acc: 0.8966\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.5305 - acc: 0.9025\n",
      "Epoch 9: loss improved from 0.56262 to 0.53061, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.5306 - acc: 0.9025\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.9080\n",
      "Epoch 10: loss improved from 0.53061 to 0.50152, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.5015 - acc: 0.9079\n",
      "Q: 성형 견적 받아 볼까\n",
      "A: 저 도 듣고 싶어요 \n",
      "\n",
      "\n",
      "Q: 수박 맛있어\n",
      "A: 저 도 듣고 싶네요 \n",
      "\n",
      "\n",
      "Q: 명품 선물 부담스러울까\n",
      "A: 저 도 보고 싶어요 \n",
      "\n",
      "\n",
      "processing epoch: 21...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4749 - acc: 0.9125\n",
      "Epoch 1: loss improved from 0.50152 to 0.47494, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.4749 - acc: 0.9125\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.4500 - acc: 0.9182\n",
      "Epoch 2: loss improved from 0.47494 to 0.44964, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.4496 - acc: 0.9182\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4256 - acc: 0.9226\n",
      "Epoch 3: loss improved from 0.44964 to 0.42562, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.4256 - acc: 0.9226\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.4046 - acc: 0.9267\n",
      "Epoch 4: loss improved from 0.42562 to 0.40441, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.4044 - acc: 0.9267\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.3865 - acc: 0.9308\n",
      "Epoch 5: loss improved from 0.40441 to 0.38632, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.3863 - acc: 0.9308\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.9339\n",
      "Epoch 6: loss improved from 0.38632 to 0.36637, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.3664 - acc: 0.9339\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.9371\n",
      "Epoch 7: loss improved from 0.36637 to 0.34952, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.3495 - acc: 0.9371\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3333 - acc: 0.9400\n",
      "Epoch 8: loss improved from 0.34952 to 0.33329, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.3333 - acc: 0.9400\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.9427\n",
      "Epoch 9: loss improved from 0.33329 to 0.32030, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.3203 - acc: 0.9427\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3075 - acc: 0.9449\n",
      "Epoch 10: loss improved from 0.32030 to 0.30746, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.3075 - acc: 0.9449\n",
      "Q: 아무래도 전공 을 잘못 골랐나 봐\n",
      "A: 저 는 차갑게 \n",
      "\n",
      "\n",
      "Q: 비 오네\n",
      "A: 저 도 몰랐어요 \n",
      "\n",
      "\n",
      "Q: 내 가 왜 해야하는지 모르겠어\n",
      "A: 많이 만나 보세요 \n",
      "\n",
      "\n",
      "processing epoch: 31...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2954 - acc: 0.9470\n",
      "Epoch 1: loss improved from 0.30746 to 0.29535, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.2953 - acc: 0.9470\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2832 - acc: 0.9493\n",
      "Epoch 2: loss improved from 0.29535 to 0.28333, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.2833 - acc: 0.9493\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2735 - acc: 0.9505\n",
      "Epoch 3: loss improved from 0.28333 to 0.27391, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.2739 - acc: 0.9505\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2647 - acc: 0.9520\n",
      "Epoch 4: loss improved from 0.27391 to 0.26520, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.2652 - acc: 0.9518\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9532\n",
      "Epoch 5: loss improved from 0.26520 to 0.25719, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.2572 - acc: 0.9532\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2481 - acc: 0.9544\n",
      "Epoch 6: loss improved from 0.25719 to 0.24815, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.2481 - acc: 0.9544\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2409 - acc: 0.9563\n",
      "Epoch 7: loss improved from 0.24815 to 0.24090, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.2409 - acc: 0.9563\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2350 - acc: 0.9568\n",
      "Epoch 8: loss improved from 0.24090 to 0.23521, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.2352 - acc: 0.9567\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2303 - acc: 0.9574\n",
      "Epoch 9: loss improved from 0.23521 to 0.23038, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.2304 - acc: 0.9574\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2284 - acc: 0.9572\n",
      "Epoch 10: loss improved from 0.23038 to 0.22822, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.2282 - acc: 0.9572\n",
      "Q: 나 잘 하고 있는 건지 모르겠어\n",
      "A: 잘 할 수 있을 거 예요 \n",
      "\n",
      "\n",
      "Q: 놀아줘 놀아줘\n",
      "A: 좀 더 기다려주세요 \n",
      "\n",
      "\n",
      "Q: 교통사고 났었어\n",
      "A: 보험 처리 하세요 \n",
      "\n",
      "\n",
      "processing epoch: 41...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2193 - acc: 0.9587\n",
      "Epoch 1: loss improved from 0.22822 to 0.21934, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.2193 - acc: 0.9587\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2127 - acc: 0.9596\n",
      "Epoch 2: loss improved from 0.21934 to 0.21280, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.2128 - acc: 0.9595\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2075 - acc: 0.9599\n",
      "Epoch 3: loss improved from 0.21280 to 0.20750, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.2075 - acc: 0.9599\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2020 - acc: 0.9610\n",
      "Epoch 4: loss improved from 0.20750 to 0.20199, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.2020 - acc: 0.9610\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1973 - acc: 0.9612\n",
      "Epoch 5: loss improved from 0.20199 to 0.19732, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.1973 - acc: 0.9612\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1929 - acc: 0.9621\n",
      "Epoch 6: loss improved from 0.19732 to 0.19301, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.1930 - acc: 0.9621\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9627\n",
      "Epoch 7: loss improved from 0.19301 to 0.18771, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.1877 - acc: 0.9627\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1841 - acc: 0.9630\n",
      "Epoch 8: loss improved from 0.18771 to 0.18425, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.1842 - acc: 0.9629\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1809 - acc: 0.9636\n",
      "Epoch 9: loss improved from 0.18425 to 0.18087, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.1809 - acc: 0.9636\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9641\n",
      "Epoch 10: loss improved from 0.18087 to 0.17710, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.1771 - acc: 0.9641\n",
      "Q: 스트레스 받아\n",
      "A: 저 도 즐거워요 \n",
      "\n",
      "\n",
      "Q: 바다 놀러 가고 싶다\n",
      "A: 같이 가자고 말 해보세요 \n",
      "\n",
      "\n",
      "Q: 담배 왤케 비싸\n",
      "A: 좋은 습관 이에요 \n",
      "\n",
      "\n",
      "processing epoch: 51...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9643\n",
      "Epoch 1: loss improved from 0.17710 to 0.17328, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.1733 - acc: 0.9643\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9646\n",
      "Epoch 2: loss improved from 0.17328 to 0.16900, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.1690 - acc: 0.9646\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9645\n",
      "Epoch 3: loss improved from 0.16900 to 0.16818, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.1682 - acc: 0.9645\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1629 - acc: 0.9655\n",
      "Epoch 4: loss improved from 0.16818 to 0.16291, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 645s 3s/step - loss: 0.1629 - acc: 0.9655\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1597 - acc: 0.9658\n",
      "Epoch 5: loss improved from 0.16291 to 0.15975, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.1598 - acc: 0.9657\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1561 - acc: 0.9665\n",
      "Epoch 6: loss improved from 0.15975 to 0.15619, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1562 - acc: 0.9665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.9666\n",
      "Epoch 7: loss improved from 0.15619 to 0.15311, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.1531 - acc: 0.9666\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1643 - acc: 0.9647\n",
      "Epoch 8: loss did not improve from 0.15311\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.1643 - acc: 0.9647\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1498 - acc: 0.9670\n",
      "Epoch 9: loss improved from 0.15311 to 0.14978, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1498 - acc: 0.9670\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.9674\n",
      "Epoch 10: loss improved from 0.14978 to 0.14669, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.1467 - acc: 0.9674\n",
      "Q: 갑자기 눈물 나\n",
      "A: 마음 이 아픈가요 \n",
      "\n",
      "\n",
      "Q: 날씨 왜 이렇게 춥냐\n",
      "A: 저 도 요 \n",
      "\n",
      "\n",
      "Q: 세탁소 가기 귀찮네\n",
      "A: 스타 일러 사세요 \n",
      "\n",
      "\n",
      "processing epoch: 61...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1441 - acc: 0.9675\n",
      "Epoch 1: loss improved from 0.14669 to 0.14409, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.1441 - acc: 0.9675\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1384 - acc: 0.9684\n",
      "Epoch 2: loss improved from 0.14409 to 0.13844, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.1384 - acc: 0.9684\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1349 - acc: 0.9689\n",
      "Epoch 3: loss improved from 0.13844 to 0.13493, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1349 - acc: 0.9689\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1333 - acc: 0.9691\n",
      "Epoch 4: loss improved from 0.13493 to 0.13330, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.1333 - acc: 0.9691\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1287 - acc: 0.9700\n",
      "Epoch 5: loss improved from 0.13330 to 0.12873, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 72ms/step - loss: 0.1287 - acc: 0.9700\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1286 - acc: 0.9699\n",
      "Epoch 6: loss improved from 0.12873 to 0.12861, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1286 - acc: 0.9699\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1232 - acc: 0.9709\n",
      "Epoch 7: loss improved from 0.12861 to 0.12324, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.1232 - acc: 0.9709\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9707\n",
      "Epoch 8: loss improved from 0.12324 to 0.12272, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.1227 - acc: 0.9707\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1204 - acc: 0.9710\n",
      "Epoch 9: loss improved from 0.12272 to 0.12038, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.1204 - acc: 0.9710\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9700\n",
      "Epoch 10: loss did not improve from 0.12038\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.1232 - acc: 0.9700\n",
      "Q: 빙빙 돌려서 말 하지 말고 다이렉트 로 말 했으면\n",
      "A: 친구 들 이 보고싶었나 봐요 \n",
      "\n",
      "\n",
      "Q: 나 랑 있는게 힘들었나 봐\n",
      "A: 상황 이 솔솔 와 서 수면 양말 인가 봐요 \n",
      "\n",
      "\n",
      "Q: 미용실 에서 너무 티 안 나게 머리 를 잘라 줬어\n",
      "A: 다시 방문 해보세요 \n",
      "\n",
      "\n",
      "processing epoch: 71...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1158 - acc: 0.9717\n",
      "Epoch 1: loss improved from 0.12038 to 0.11583, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.1158 - acc: 0.9717\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1113 - acc: 0.9722\n",
      "Epoch 2: loss improved from 0.11583 to 0.11131, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.1113 - acc: 0.9722\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1108 - acc: 0.9722\n",
      "Epoch 3: loss improved from 0.11131 to 0.11079, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.1108 - acc: 0.9722\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9733\n",
      "Epoch 4: loss improved from 0.11079 to 0.10664, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 64ms/step - loss: 0.1066 - acc: 0.9733\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9735\n",
      "Epoch 5: loss improved from 0.10664 to 0.10435, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.1044 - acc: 0.9735\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1003 - acc: 0.9749\n",
      "Epoch 6: loss improved from 0.10435 to 0.10036, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.1004 - acc: 0.9749\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0982 - acc: 0.9754\n",
      "Epoch 7: loss improved from 0.10036 to 0.09823, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.0982 - acc: 0.9754\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0943 - acc: 0.9759\n",
      "Epoch 8: loss improved from 0.09823 to 0.09426, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0943 - acc: 0.9759\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0925 - acc: 0.9758\n",
      "Epoch 9: loss improved from 0.09426 to 0.09254, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0925 - acc: 0.9758\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9764\n",
      "Epoch 10: loss improved from 0.09254 to 0.09023, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0902 - acc: 0.9765\n",
      "Q: 로또 좀 됐으면\n",
      "A: 콕 집어서 물어보세요 \n",
      "\n",
      "\n",
      "Q: 바람 많이 부네\n",
      "A: 감기 조심하세요 \n",
      "\n",
      "\n",
      "Q: 거지 같이 일해 놓고 갔어\n",
      "A: 그걸 깨닫다니 대단하시군요 \n",
      "\n",
      "\n",
      "processing epoch: 81...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9771\n",
      "Epoch 1: loss improved from 0.09023 to 0.08696, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0870 - acc: 0.9771\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9780\n",
      "Epoch 2: loss improved from 0.08696 to 0.08507, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.0851 - acc: 0.9780\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0825 - acc: 0.9782\n",
      "Epoch 3: loss improved from 0.08507 to 0.08248, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.0825 - acc: 0.9782\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0819 - acc: 0.9785\n",
      "Epoch 4: loss improved from 0.08248 to 0.08193, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0819 - acc: 0.9785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0790 - acc: 0.9790\n",
      "Epoch 5: loss improved from 0.08193 to 0.07901, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0790 - acc: 0.9790\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0780 - acc: 0.9792\n",
      "Epoch 6: loss improved from 0.07901 to 0.07798, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0780 - acc: 0.9792\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9803\n",
      "Epoch 7: loss improved from 0.07798 to 0.07445, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0744 - acc: 0.9803\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9816\n",
      "Epoch 8: loss improved from 0.07445 to 0.06943, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0694 - acc: 0.9816\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9817\n",
      "Epoch 9: loss improved from 0.06943 to 0.06887, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0689 - acc: 0.9817\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9813\n",
      "Epoch 10: loss improved from 0.06887 to 0.06806, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0681 - acc: 0.9813\n",
      "Q: 문신 하고 싶은데 후회 할까\n",
      "A: 나중 에 후회 할 수도 있을 것 같아요 \n",
      "\n",
      "\n",
      "Q: 그 사람 이 나 좋아해 줬으면 좋겠다\n",
      "A: 휴가 를 붙 여보세요 \n",
      "\n",
      "\n",
      "Q: 넘 많이 먹었다\n",
      "A: 산책 좀 해야 겠네여 \n",
      "\n",
      "\n",
      "processing epoch: 91...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0666 - acc: 0.9824\n",
      "Epoch 1: loss improved from 0.06806 to 0.06664, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.0666 - acc: 0.9824\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9832\n",
      "Epoch 2: loss improved from 0.06664 to 0.06407, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0641 - acc: 0.9832\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9837\n",
      "Epoch 3: loss improved from 0.06407 to 0.06170, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 64ms/step - loss: 0.0617 - acc: 0.9838\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0592 - acc: 0.9844\n",
      "Epoch 4: loss improved from 0.06170 to 0.05919, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0592 - acc: 0.9844\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9845\n",
      "Epoch 5: loss improved from 0.05919 to 0.05842, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0584 - acc: 0.9845\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9854\n",
      "Epoch 6: loss improved from 0.05842 to 0.05542, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0554 - acc: 0.9854\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9854\n",
      "Epoch 7: loss improved from 0.05542 to 0.05507, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0551 - acc: 0.9854\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0548 - acc: 0.9855\n",
      "Epoch 8: loss improved from 0.05507 to 0.05481, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 72ms/step - loss: 0.0548 - acc: 0.9855\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9857\n",
      "Epoch 9: loss improved from 0.05481 to 0.05375, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0538 - acc: 0.9857\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0505 - acc: 0.9867\n",
      "Epoch 10: loss improved from 0.05375 to 0.05049, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0505 - acc: 0.9867\n",
      "Q: 기능 좀 알려줘 봐 봐\n",
      "A: 시원하게 지낸 값 이 죠 \n",
      "\n",
      "\n",
      "Q: 새 신발 신었는데 비 와\n",
      "A: 얼른 실내 로 들어가세요 \n",
      "\n",
      "\n",
      "Q: 너무 마른 거 같아\n",
      "A: 적당해요 \n",
      "\n",
      "\n",
      "processing epoch: 101...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0476 - acc: 0.9874\n",
      "Epoch 1: loss improved from 0.05049 to 0.04757, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0476 - acc: 0.9874\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0458 - acc: 0.9879\n",
      "Epoch 2: loss improved from 0.04757 to 0.04581, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0458 - acc: 0.9879\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0434 - acc: 0.9886\n",
      "Epoch 3: loss improved from 0.04581 to 0.04335, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0434 - acc: 0.9886\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9886\n",
      "Epoch 4: loss improved from 0.04335 to 0.04234, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0423 - acc: 0.9886\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0429 - acc: 0.9886\n",
      "Epoch 5: loss did not improve from 0.04234\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0429 - acc: 0.9886\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9881\n",
      "Epoch 6: loss did not improve from 0.04234\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0446 - acc: 0.9881\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9893\n",
      "Epoch 7: loss improved from 0.04234 to 0.04023, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0402 - acc: 0.9893\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0386 - acc: 0.9897\n",
      "Epoch 8: loss improved from 0.04023 to 0.03861, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0386 - acc: 0.9897\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9902\n",
      "Epoch 9: loss improved from 0.03861 to 0.03753, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0375 - acc: 0.9902\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9915\n",
      "Epoch 10: loss improved from 0.03753 to 0.03348, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0335 - acc: 0.9915\n",
      "Q: 나 화장 을 너무 못 해\n",
      "A: 도전 해 봐도 좋을 거 같아요 \n",
      "\n",
      "\n",
      "Q: 게임 하고 싶어\n",
      "A: 게임 하세요 \n",
      "\n",
      "\n",
      "Q: 복근 좀 생겼으면\n",
      "A: 윗몸일으키기 시작 하세요 \n",
      "\n",
      "\n",
      "processing epoch: 111...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0354 - acc: 0.9907\n",
      "Epoch 1: loss did not improve from 0.03348\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0354 - acc: 0.9907\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9916\n",
      "Epoch 2: loss did not improve from 0.03348\n",
      "188/188 [==============================] - 12s 64ms/step - loss: 0.0342 - acc: 0.9916\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9923\n",
      "Epoch 3: loss improved from 0.03348 to 0.03100, saving model to model/chatmodel-attention-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 12s 64ms/step - loss: 0.0310 - acc: 0.9923\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9923\n",
      "Epoch 4: loss did not improve from 0.03100\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0311 - acc: 0.9923\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0275 - acc: 0.9932\n",
      "Epoch 5: loss improved from 0.03100 to 0.02750, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0275 - acc: 0.9932\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.9933\n",
      "Epoch 6: loss improved from 0.02750 to 0.02686, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0269 - acc: 0.9933\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9933\n",
      "Epoch 7: loss improved from 0.02686 to 0.02676, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.0268 - acc: 0.9933\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0254 - acc: 0.9937\n",
      "Epoch 8: loss improved from 0.02676 to 0.02536, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0254 - acc: 0.9937\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0237 - acc: 0.9942\n",
      "Epoch 9: loss improved from 0.02536 to 0.02367, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0237 - acc: 0.9942\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9946\n",
      "Epoch 10: loss improved from 0.02367 to 0.02278, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.0228 - acc: 0.9945\n",
      "Q: 나 그동안 뭐 한거니\n",
      "A: 바람 좀 쐬고 오시 면 좋은텐데 \n",
      "\n",
      "\n",
      "Q: 상의 도 없이 왜 그런 결정 을 하지\n",
      "A: 이야기 를 하지 않고 결정 했나 봐요 \n",
      "\n",
      "\n",
      "Q: 스키장 알바 재밌대\n",
      "A: 실컷 스키 탈 수 있을 것 같아요 \n",
      "\n",
      "\n",
      "processing epoch: 121...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9946\n",
      "Epoch 1: loss improved from 0.02278 to 0.02264, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.0226 - acc: 0.9946\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0231 - acc: 0.9941\n",
      "Epoch 2: loss did not improve from 0.02264\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0231 - acc: 0.9941\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9950\n",
      "Epoch 3: loss improved from 0.02264 to 0.02099, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0210 - acc: 0.9950\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9949\n",
      "Epoch 4: loss did not improve from 0.02099\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0218 - acc: 0.9949\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0221 - acc: 0.9945\n",
      "Epoch 5: loss did not improve from 0.02099\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0221 - acc: 0.9945\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9951\n",
      "Epoch 6: loss improved from 0.02099 to 0.02029, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0203 - acc: 0.9951\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0196 - acc: 0.9954\n",
      "Epoch 7: loss improved from 0.02029 to 0.01956, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0196 - acc: 0.9954\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9953\n",
      "Epoch 8: loss improved from 0.01956 to 0.01927, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0193 - acc: 0.9953\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9953\n",
      "Epoch 9: loss improved from 0.01927 to 0.01846, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0185 - acc: 0.9953\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9962\n",
      "Epoch 10: loss improved from 0.01846 to 0.01642, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0164 - acc: 0.9962\n",
      "Q: 로또 번호 알려줘\n",
      "A: 알 면 제 가 하죠 \n",
      "\n",
      "\n",
      "Q: 상 좀 받아야 할텐데\n",
      "A: 다음 에는 받을 수 있을 거 예요 \n",
      "\n",
      "\n",
      "Q: 몇 시간 동안 안 와\n",
      "A: 기다리는 동안 많은 생각 이 들었겠네요 \n",
      "\n",
      "\n",
      "processing epoch: 131...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0167 - acc: 0.9962\n",
      "Epoch 1: loss did not improve from 0.01642\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0167 - acc: 0.9962\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0165 - acc: 0.9961\n",
      "Epoch 2: loss did not improve from 0.01642\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.0165 - acc: 0.9961\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9962\n",
      "Epoch 3: loss improved from 0.01642 to 0.01564, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0156 - acc: 0.9962\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0164 - acc: 0.9961\n",
      "Epoch 4: loss did not improve from 0.01564\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.0164 - acc: 0.9961\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9962\n",
      "Epoch 5: loss did not improve from 0.01564\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0159 - acc: 0.9962\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 6: loss improved from 0.01564 to 0.01519, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0142 - acc: 0.9967\n",
      "Epoch 7: loss improved from 0.01519 to 0.01425, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0142 - acc: 0.9967\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9959\n",
      "Epoch 8: loss did not improve from 0.01425\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0159 - acc: 0.9959\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9967\n",
      "Epoch 9: loss improved from 0.01425 to 0.01384, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0138 - acc: 0.9966\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9973\n",
      "Epoch 10: loss improved from 0.01384 to 0.01191, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0119 - acc: 0.9973\n",
      "Q: 방법 도 안 말 해주고 그냥 열심히 하래\n",
      "A: 모르는걸 알려면 노력 해야죠 \n",
      "\n",
      "\n",
      "Q: 아이고 소리 가 절로 나온다\n",
      "A: 많이 지쳤나 봐요 \n",
      "\n",
      "\n",
      "Q: 도서관 에서 괜찮은 사람 봤어\n",
      "A: 도서관 도 다니시나 봐요 \n",
      "\n",
      "\n",
      "processing epoch: 141...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9924\n",
      "Epoch 1: loss did not improve from 0.01191\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0361 - acc: 0.9924\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9951\n",
      "Epoch 2: loss did not improve from 0.01191\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0202 - acc: 0.9951\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9968\n",
      "Epoch 3: loss did not improve from 0.01191\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0140 - acc: 0.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0116 - acc: 0.9975\n",
      "Epoch 4: loss improved from 0.01191 to 0.01159, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0116 - acc: 0.9975\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0098 - acc: 0.9982\n",
      "Epoch 5: loss improved from 0.01159 to 0.00978, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0098 - acc: 0.9982\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9979\n",
      "Epoch 6: loss improved from 0.00978 to 0.00923, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0092 - acc: 0.9979\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9980\n",
      "Epoch 7: loss improved from 0.00923 to 0.00861, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0086 - acc: 0.9980\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0086 - acc: 0.9981\n",
      "Epoch 8: loss improved from 0.00861 to 0.00860, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0086 - acc: 0.9981\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0107 - acc: 0.9976\n",
      "Epoch 9: loss did not improve from 0.00860\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0107 - acc: 0.9976\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0089 - acc: 0.9981\n",
      "Epoch 10: loss did not improve from 0.00860\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0088 - acc: 0.9981\n",
      "Q: 나 좋아하는 것 같아\n",
      "A: 호의 인지 호감 인지 헷갈리나요 \n",
      "\n",
      "\n",
      "Q: 김치볶음밥 먹어야지\n",
      "A: 맛있는 식사 시간 되시길 바랄게요 \n",
      "\n",
      "\n",
      "Q: 뭘 해도 귀여워\n",
      "A: 사랑 하 나 봅니다 \n",
      "\n",
      "\n",
      "processing epoch: 151...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9982\n",
      "Epoch 1: loss improved from 0.00860 to 0.00795, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.0080 - acc: 0.9982\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9982\n",
      "Epoch 2: loss did not improve from 0.00795\n",
      "188/188 [==============================] - 12s 65ms/step - loss: 0.0082 - acc: 0.9982\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9981\n",
      "Epoch 3: loss did not improve from 0.00795\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0092 - acc: 0.9981\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9971\n",
      "Epoch 4: loss did not improve from 0.00795\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0118 - acc: 0.9971\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9976\n",
      "Epoch 5: loss did not improve from 0.00795\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0099 - acc: 0.9976\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9982\n",
      "Epoch 6: loss did not improve from 0.00795\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0080 - acc: 0.9982\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9980\n",
      "Epoch 7: loss did not improve from 0.00795\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0082 - acc: 0.9980\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 8: loss improved from 0.00795 to 0.00750, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0075 - acc: 0.9983\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9985\n",
      "Epoch 9: loss improved from 0.00750 to 0.00695, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0069 - acc: 0.9985\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9985\n",
      "Epoch 10: loss improved from 0.00695 to 0.00688, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.0069 - acc: 0.9985\n",
      "Q: 감기 인거 같 애\n",
      "A: 병원 가세 요 \n",
      "\n",
      "\n",
      "Q: 내일 은 비 왔으면 좋겠다\n",
      "A: 기우제 를 지내 봅시다 \n",
      "\n",
      "\n",
      "Q: 믿는 게 아니었어\n",
      "A: 한번 더 기회 를 주세요 \n",
      "\n",
      "\n",
      "processing epoch: 161...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9982\n",
      "Epoch 1: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0084 - acc: 0.9982\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9981\n",
      "Epoch 2: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0080 - acc: 0.9981\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9982\n",
      "Epoch 3: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0073 - acc: 0.9982\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9976\n",
      "Epoch 4: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0098 - acc: 0.9976\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9979\n",
      "Epoch 5: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0088 - acc: 0.9979\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 6: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 7: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 8: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9979\n",
      "Epoch 9: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0091 - acc: 0.9979\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 10: loss did not improve from 0.00688\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0070 - acc: 0.9983\n",
      "Q: 교보 문고 왔어\n",
      "A: 마음 에 드는 책 을 잘 찾아보세요 \n",
      "\n",
      "\n",
      "Q: 사진 잘 찍고 싶어\n",
      "A: 많이 찍다 보면 조금씩 실력 이 늘거예요 \n",
      "\n",
      "\n",
      "Q: 감정 컨트롤 이 안 돼\n",
      "A: 그건 습관 이에요 \n",
      "\n",
      "\n",
      "processing epoch: 171...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
      "Epoch 1: loss improved from 0.00688 to 0.00679, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.0068 - acc: 0.9984\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 2: loss improved from 0.00679 to 0.00675, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 66ms/step - loss: 0.0068 - acc: 0.9984\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 3: loss did not improve from 0.00675\n",
      "188/188 [==============================] - 12s 66ms/step - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 4: loss improved from 0.00675 to 0.00660, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0066 - acc: 0.9985\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9986\n",
      "Epoch 5: loss improved from 0.00660 to 0.00655, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0065 - acc: 0.9985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 6: loss did not improve from 0.00655\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0070 - acc: 0.9983\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 7: loss improved from 0.00655 to 0.00575, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.0058 - acc: 0.9986\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 8: loss did not improve from 0.00575\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0060 - acc: 0.9985\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0073 - acc: 0.9981\n",
      "Epoch 9: loss did not improve from 0.00575\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0073 - acc: 0.9981\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9984\n",
      "Epoch 10: loss did not improve from 0.00575\n",
      "188/188 [==============================] - 13s 72ms/step - loss: 0.0065 - acc: 0.9984\n",
      "Q: 생각 없이 말 했어\n",
      "A: 생각 하고 말 하세요 \n",
      "\n",
      "\n",
      "Q: 골프 치러 가야 돼\n",
      "A: 시간 내서 가보세요 \n",
      "\n",
      "\n",
      "Q: 남친 어디서 만나\n",
      "A: 원하는 사람 이 있는 장소 에 가보세요 \n",
      "\n",
      "\n",
      "processing epoch: 181...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 1: loss improved from 0.00575 to 0.00499, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 2: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9981\n",
      "Epoch 3: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0071 - acc: 0.9981\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 4: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0067 - acc: 0.9984\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 5: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0058 - acc: 0.9984\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9981\n",
      "Epoch 6: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0070 - acc: 0.9981\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9982\n",
      "Epoch 7: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0067 - acc: 0.9982\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 8: loss did not improve from 0.00499\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0054 - acc: 0.9986\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0048 - acc: 0.9987\n",
      "Epoch 9: loss improved from 0.00499 to 0.00483, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.0048 - acc: 0.9987\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 10: loss improved from 0.00483 to 0.00444, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0044 - acc: 0.9988\n",
      "Q: 나 점점 괴물 이 되고 있어\n",
      "A: 그렇지 않아요 \n",
      "\n",
      "\n",
      "Q: 놀이동산 가자고 해볼까\n",
      "A: 놀이동산 은 다 좋아할 거 예요 \n",
      "\n",
      "\n",
      "Q: 별 보러 가고 싶다\n",
      "A: 로맨틱 하네요 \n",
      "\n",
      "\n",
      "processing epoch: 191...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 1: loss improved from 0.00444 to 0.00429, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.0043 - acc: 0.9989\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9989\n",
      "Epoch 2: loss improved from 0.00429 to 0.00409, saving model to model/chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.0041 - acc: 0.9989\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 3: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 4: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0042 - acc: 0.9988\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 5: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 6: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0049 - acc: 0.9987\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0068 - acc: 0.9985\n",
      "Epoch 7: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0068 - acc: 0.9985\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n",
      "Epoch 8: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0054 - acc: 0.9987\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9986\n",
      "Epoch 9: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 13s 71ms/step - loss: 0.0051 - acc: 0.9986\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n",
      "Epoch 10: loss did not improve from 0.00409\n",
      "188/188 [==============================] - 13s 67ms/step - loss: 0.0047 - acc: 0.9989\n",
      "Q: 과식 했나 봐\n",
      "A: 과식 은 금물 이에요 \n",
      "\n",
      "\n",
      "Q: 막말 최악 이지\n",
      "A: 말로 하는 상처 는 지울 수도 없죠 \n",
      "\n",
      "\n",
      "Q: 먼지 가 너무 많아\n",
      "A: 얼른 청소 하세요 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
    "    chat.fit([question_padded, answer_in_padded],\n",
    "                answer_out_one_hot,\n",
    "                epochs=10,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[checkpoint]\n",
    "               )\n",
    "    # 랜덤한 샘플 번호 추출\n",
    "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
    "\n",
    "    # 예측 성능 테스트\n",
    "    for idx in samples:\n",
    "        question_inputs = question_padded[idx]\n",
    "        # 문장 예측\n",
    "        results = make_prediction(chat, np.expand_dims(question_inputs, 0))\n",
    "        \n",
    "        # 변환된 인덱스를 문장으로 변환\n",
    "        results = convert_index_to_text(results, END_TOKEN)\n",
    "        \n",
    "        print(f'Q: {questions[idx]}')\n",
    "        print(f'A: {results}\\n')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a162035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8584cb",
   "metadata": {},
   "source": [
    "## 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc8ed2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163, 229, 352,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자연어 (질문 입력) 대한 전처리 함수\n",
    "def make_question(sentence):\n",
    "    sentence = clean_and_morph(sentence)\n",
    "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "    return question_padded\n",
    "\n",
    "make_question('오늘 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e32649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변을 리턴해주는 함수\n",
    "def run_chatbot(question):\n",
    "    question_inputs = make_question(question)\n",
    "    results = make_prediction(chat, question_inputs)\n",
    "    results = convert_index_to_text(results, END_TOKEN)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "448ab6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 말을 걸어 보세요!\n",
      "20대 여자 칵테일 추천\n",
      ">> 챗봇 응답: 저 는 롱티 좋아해요 \n",
      "<< 말을 걸어 보세요!\n",
      "남자 칵테일\n",
      ">> 챗봇 응답: 다음 에는 다를거예요 \n",
      "<< 말을 걸어 보세요!\n",
      "어떤 보드카\n",
      ">> 챗봇 응답: 가격 에 따라 천차만별 이지만 국내 에서 많이 찾는 보드카 로는 앱솔 루트 스미 노프 벨루가 등 이 있어요 \n",
      "<< 말을 걸어 보세요!\n",
      "q\n"
     ]
    }
   ],
   "source": [
    "# 콘솔에서 테스트\n",
    "while True:\n",
    "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
    "    if user_input == 'q':\n",
    "        break\n",
    "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829f773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.20.105:9000/ (Press CTRL+C to quit)\n",
      "192.168.20.105 - - [27/Apr/2022 08:59:31] \"GET /chatbot?question=어떤%20보드카 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어떤 보드카\n",
      ">> 챗봇 응답: 가격 에 따라 천차만별 이지만 국내 에서 많이 찾는 보드카 로는 앱솔 루트 스미 노프 벨루가 등 이 있어요 \n",
      "여자가 좋아하는 칵테일여자가 좋아하는 칵테일여자가 좋아하는 칵테일여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:19:07] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:07] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:07] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:07] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:07] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:07] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      "여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n",
      "여자가 좋아하는 칵테일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      "여자가 좋아하는 칵테일여자가 좋아하는 칵테일\n",
      "\n",
      "여자가 좋아하는 칵테일\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:08] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 챗봇 응답: 저 는 주 당 이에요 \n",
      ">> 챗봇 응답: 저 는 주 당 이에요 >> 챗봇 응답: 저 는 주 당 이에요 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:19:22] \"GET /chatbot?question=13 HTTP/1.1\" 200 -\n",
      "192.168.20.97 - - [27/Apr/2022 09:19:22] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      ">> 챗봇 응답: 살아있음 그 자체 가 좋은 거 예요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:19:32] \"GET /chatbot?question=여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여자가 좋아하는 칵테일\n",
      ">> 챗봇 응답: 저 는 주 당 이에요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:22:31] \"GET /chatbot?question=20대+여자가+좋아하는+칵테일 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20대 여자가 좋아하는 칵테일\n",
      ">> 챗봇 응답: 저 는 롱티 좋아해요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:25:17] \"GET /chatbot?question=맛있는+칵테일+추천해줘 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맛있는 칵테일 추천해줘\n",
      ">> 챗봇 응답: 어떤 맛 좋아하세요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.20.97 - - [27/Apr/2022 09:32:48] \"GET /chatbot?question=어떤+보드카가+좋아%3F HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어떤 보드카가 좋아?\n",
      ">> 챗봇 응답: 가격 에 따라 천차만별 이지만 국내 에서 많이 찾는 보드카 로는 앱솔 루트 스미 노프 벨루가 등 이 있어요 \n"
     ]
    }
   ],
   "source": [
    "# 웹 서비스를 위한 배포\n",
    "from flask import Flask, request\n",
    "from flask import jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "#시작 요청이 왔을 때 아래 코드를 수행\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def main():\n",
    "    return 'Hello Chatbot'\n",
    "\n",
    "@app.route('/chatbot', methods=['POST', 'GET'])\n",
    "def chatbot():\n",
    "    print(request.args[\"question\"])\n",
    "    answer = run_chatbot(request.args[\"question\"])\n",
    "    print('>> 챗봇 응답: {}'.format(answer))\n",
    "    response = {'answer': answer}\n",
    "    return jsonify(response)\n",
    "\n",
    "#서버 구동\n",
    "app.run('0.0.0.0', port=9000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9464915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
