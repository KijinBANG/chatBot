{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb226210",
   "metadata": {},
   "source": [
    "# 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f362af65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통코드\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b1f64",
   "metadata": {},
   "source": [
    "## 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7a3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n",
    "corpus = pd.read_csv('./ChatbotData.csv')\n",
    "\n",
    "# 2,000개 데이터 셋만 활용 (Google Colab 일 경우 3,000개에서는 메모리 오버되는 현상 발생)\n",
    "texts = []\n",
    "pairs = []\n",
    "for i, (text, pair) in enumerate(zip(corpus['Q'], corpus['A'])):\n",
    "    texts.append(text)\n",
    "    pairs.append(pair)\n",
    "    \n",
    "    #메모리가 부족하면 데이터 개수 조절\n",
    "    if i >= 3000: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d33721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12시 땡!', '하루가 또 가네요.'),\n",
       " ('1지망 학교 떨어졌어', '위로해 드립니다.'),\n",
       " ('3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
       " ('3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
       " ('PPL 심하네', '눈살이 찌푸려지죠.'),\n",
       " ('SD카드 망가졌어', '다시 새로 사는 게 마음 편해요.'),\n",
       " ('SD카드 안돼', '다시 새로 사는 게 마음 편해요.'),\n",
       " ('SNS 맞팔 왜 안하지ㅠㅠ', '잘 모르고 있을 수도 있어요.'),\n",
       " ('SNS 시간낭비인 거 아는데 매일 하는 중', '시간을 정하고 해보세요.'),\n",
       " ('SNS 시간낭비인데 자꾸 보게됨', '시간을 정하고 해보세요.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question와 answer 데이터 확인\n",
    "list(zip(texts, pairs))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653510ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a8af66",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d123693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_sentence(sentence):\n",
    "#     # 한글, 숫자, 영문 대/소문자를 제외한 모든 문자는 제거합니다.\n",
    "#     sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣ㅣa-zA-Z ]',r'', sentence)\n",
    "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
    "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣]',r'', sentence)\n",
    "    return sentence\n",
    "\n",
    "# 전처리 함수 테스트\n",
    "print(clean_sentence('안녕하세요~:)'))\n",
    "print(clean_sentence('TensorFlow^@^%#@!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08099864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한글 형태소 분석\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "def process_morph(sentence):\n",
    "    return ' '.join(okt.morphs(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56562a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#챗봇을 위해 필요한 데이터 생성 함수\n",
    "def clean_and_morph(sentence, is_question=True):\n",
    "    # 한글 문장 전처리\n",
    "    sentence = clean_sentence(sentence)\n",
    "    # 형태소 변환\n",
    "    sentence = process_morph(sentence)\n",
    "    # Question 인 경우, Answer인 경우를 분기하여 처리\n",
    "    # Answer에는 시작 과 종료 기호 추가\n",
    "    if is_question:\n",
    "        return sentence\n",
    "    else:\n",
    "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
    "        return ('<START> ' + sentence, sentence + ' <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552cfe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#챗봇을 위한 데이터 생성\n",
    "def preprocess(texts, pairs):\n",
    "    questions = []\n",
    "    answer_in = []\n",
    "    answer_out = []\n",
    "\n",
    "    # 질의에 대한 전처리\n",
    "    for text in texts:\n",
    "        # 전처리와 morph 수행\n",
    "        question = clean_and_morph(text, is_question=True)\n",
    "        questions.append(question)\n",
    "\n",
    "    # 답변에 대한 전처리\n",
    "    for pair in pairs:\n",
    "        # 전처리와 morph 수행\n",
    "        in_, out_ = clean_and_morph(pair, is_question=False)\n",
    "        answer_in.append(in_)\n",
    "        answer_out.append(out_)\n",
    "    \n",
    "    return questions, answer_in, answer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c92b62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡', '1 지망 학교 떨어졌어']\n",
      "['<START> 하루 가 또 가네요', '<START> 위로 해드립니다']\n",
      "['하루 가 또 가네요 <END>', '위로 해드립니다 <END>']\n"
     ]
    }
   ],
   "source": [
    "#챗봇 훈련에 필요한 데이터 생성 및 확인\n",
    "questions, answer_in, answer_out = preprocess(texts, pairs)\n",
    "\n",
    "print(questions[:2])\n",
    "print(answer_in[:2])\n",
    "print(answer_out[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed053df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문장을 하나의 문장으로 생성\n",
    "all_sentences = questions + answer_in + answer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b23777cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토크나이저 와 수치화 및 패딩\n",
    "import numpy as np\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# WARNING 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 토큰화\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(all_sentences)\n",
    "\n",
    "# 텍스트를 시퀀스로 인코딩 (texts_to_sequences)\n",
    "question_sequence = tokenizer.texts_to_sequences(questions)\n",
    "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
    "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)\n",
    "\n",
    "# 문장의 길이 맞추기 (pad_sequences)\n",
    "MAX_LENGTH = 30\n",
    "question_padded = pad_sequences(question_sequence, \n",
    "                                maxlen=MAX_LENGTH, \n",
    "                                truncating='post', \n",
    "                                padding='post')\n",
    "answer_in_padded = pad_sequences(answer_in_sequence, \n",
    "                                 maxlen=MAX_LENGTH, \n",
    "                                 truncating='post', \n",
    "                                 padding='post')\n",
    "answer_out_padded = pad_sequences(answer_out_sequence, \n",
    "                                  maxlen=MAX_LENGTH, \n",
    "                                  truncating='post', \n",
    "                                  padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc98333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OOV>\t -> \t1\n",
      "<START>\t -> \t2\n",
      "<END>\t -> \t3\n",
      "이\t -> \t4\n",
      "을\t -> \t5\n",
      "가\t -> \t6\n",
      "해보세요\t -> \t7\n",
      "요\t -> \t8\n",
      "보세요\t -> \t9\n",
      "사람\t -> \t10\n",
      "도\t -> \t11\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    print(f'{word}\\t -> \\t{idx}')\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06f5f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5360"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 개수 확인\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "945a057d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3001, 30), (3001, 30), (3001, 30))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#시퀀스 확인\n",
    "question_padded.shape, answer_in_padded.shape, answer_out_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "873736d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 5361), (30, 5361))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원핫 인코딩\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "\n",
    "#원핫 인코딩을 위한 함수\n",
    "def convert_to_one_hot(padded):\n",
    "    # 원핫인코딩 초기화\n",
    "    one_hot_vector = np.zeros((len(answer_out_padded), \n",
    "                               MAX_LENGTH, \n",
    "                               VOCAB_SIZE))\n",
    "\n",
    "    # 디코더 목표를 원핫 인코딩으로 변환\n",
    "    # 학습시 입력은 인덱스이지만 출력은 원핫 인코딩 형식임\n",
    "    for i, sequence in enumerate(answer_out_padded):\n",
    "        for j, index in enumerate(sequence):\n",
    "            one_hot_vector[i, j, index] = 1\n",
    "\n",
    "    return one_hot_vector\n",
    "\n",
    "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
    "answer_out_one_hot = convert_to_one_hot(answer_out_padded)\n",
    "answer_in_one_hot[0].shape, answer_in_one_hot[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec3750db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 index를 다시 단어로 변환\n",
    "def convert_index_to_text(indexs, end_token): \n",
    "    \n",
    "    sentence = ''\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == end_token:\n",
    "            # 끝 단어이므로 예측 중비\n",
    "            break;\n",
    "        # 사전에 존재하는 단어의 경우 단어 추가\n",
    "        if index > 0 and tokenizer.index_word[index] is not None:\n",
    "            sentence += tokenizer.index_word[index]\n",
    "        else:\n",
    "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
    "            sentence += ''\n",
    "            \n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45603a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bde5f5a0",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b29d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff0f87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력을 위한 클래스 - Attention 사용\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, \n",
    "                                   embedding_dim, \n",
    "                                   input_length=time_steps, \n",
    "                                   name='Embedding')\n",
    "        self.dropout = Dropout(0.2, name='Dropout')\n",
    "        # (attention) return_sequences=True 추가\n",
    "        self.lstm = LSTM(units, \n",
    "                         return_state=True, \n",
    "                         return_sequences=True, \n",
    "                         name='LSTM')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x)\n",
    "        # (attention) x return 추가\n",
    "        return x, [hidden_state, cell_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b76ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력을 위한 디코더\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, \n",
    "                                   embedding_dim, \n",
    "                                   input_length=time_steps, \n",
    "                                   name='Embedding')\n",
    "        self.dropout = Dropout(0.2, name='Dropout')\n",
    "        self.lstm = LSTM(units, \n",
    "                         return_state=True, \n",
    "                         return_sequences=True, \n",
    "                         name='LSTM'\n",
    "                        )\n",
    "        self.attention = Attention(name='Attention')\n",
    "        self.dense = Dense(vocab_size, \n",
    "                           activation='softmax', \n",
    "                           name='Dense')\n",
    "    \n",
    "    def call(self, inputs, initial_state):\n",
    "        # (attention) encoder_inputs 추가\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        x = self.embedding(decoder_inputs)\n",
    "        x = self.dropout(x)\n",
    "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
    "        \n",
    "        # (attention) key_value, attention_matrix 추가\n",
    "        # 이전 hidden_state의 값을 concat으로 만들어 vector를 생성합니다.        \n",
    "        key_value = tf.concat([initial_state[0][:, tf.newaxis, :], \n",
    "                               x[:, :-1, :]], axis=1)        \n",
    "        # 이전 hidden_state의 값을 concat으로 만든 vector와 encoder에서 나온 \n",
    "        # 출력 값들로 attention을 구합니다.\n",
    "        attention_matrix = self.attention([key_value, encoder_inputs])\n",
    "        # 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n",
    "        x = tf.concat([x, attention_matrix], axis=-1)\n",
    "        \n",
    "        x = self.dense(x)\n",
    "        return x, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e20a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModel(tf.keras.Model):\n",
    "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
    "        super(ChatModel, self).__init__()\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
    "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        if training:\n",
    "            encoder_inputs, decoder_inputs = inputs\n",
    "            # (attention) encoder 출력 값 수정\n",
    "            encoder_outputs, context_vector = self.encoder(encoder_inputs)\n",
    "            # (attention) decoder 입력 값 수정\n",
    "            decoder_outputs, _, _ = self.decoder((encoder_outputs, decoder_inputs), \n",
    "                                                 initial_state=context_vector)\n",
    "            return decoder_outputs\n",
    "        else:\n",
    "            x = inputs\n",
    "            # (attention) encoder 출력 값 수정\n",
    "            encoder_outputs, context_vector = self.encoder(x)\n",
    "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
    "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
    "            \n",
    "            for i in tf.range(self.time_steps):\n",
    "                decoder_output, decoder_hidden, decoder_cell = self.decoder((encoder_outputs, target_seq), \n",
    "                                                                            initial_state=context_vector)\n",
    "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
    "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
    "                results = results.write(i, decoder_output)\n",
    "                \n",
    "                if decoder_output == self.end_token:\n",
    "                    break\n",
    "                    \n",
    "                target_seq = decoder_output\n",
    "                context_vector = [decoder_hidden, decoder_cell]\n",
    "                \n",
    "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27923ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "456ef268",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4273ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "EMBEDDING_DIM = 100\n",
    "TIME_STEPS = MAX_LENGTH\n",
    "START_TOKEN = tokenizer.word_index['<START>']\n",
    "END_TOKEN = tokenizer.word_index['<END>']\n",
    "\n",
    "UNITS = 128\n",
    "\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
    "DATA_LENGTH = len(questions)\n",
    "SAMPLE_SIZE = 3\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d62cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크 포인트 생성\n",
    "checkpoint_path = 'model/chatmodel-attention-checkpoint.ckpt'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True, \n",
    "                             monitor='loss', \n",
    "                             verbose=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03681dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 생성\n",
    "chat = ChatModel(UNITS, \n",
    "                  VOCAB_SIZE, \n",
    "                  EMBEDDING_DIM, \n",
    "                  TIME_STEPS, \n",
    "                  START_TOKEN, \n",
    "                  END_TOKEN)\n",
    "\n",
    "chat.compile(optimizer='adam', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c6f809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 함수\n",
    "def make_prediction(model, question_inputs):\n",
    "    results = model(inputs=question_inputs, training=False)\n",
    "    # 변환된 인덱스를 문장으로 변환\n",
    "    results = np.asarray(results).reshape(-1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ec115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c943d37",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "814bed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing epoch: 1...\n",
      "Epoch 1/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 1.8283 - acc: 0.8215\n",
      "Epoch 1: loss improved from inf to 1.82682, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 53ms/step - loss: 1.8268 - acc: 0.8215\n",
      "Epoch 2/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 1.0596 - acc: 0.8552\n",
      "Epoch 2: loss improved from 1.82682 to 1.05972, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 11s 57ms/step - loss: 1.0597 - acc: 0.8552\n",
      "Epoch 3/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.9868 - acc: 0.8585\n",
      "Epoch 3: loss improved from 1.05972 to 0.98675, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 11s 60ms/step - loss: 0.9867 - acc: 0.8585\n",
      "Epoch 4/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.9459 - acc: 0.8600\n",
      "Epoch 4: loss improved from 0.98675 to 0.94546, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 11s 61ms/step - loss: 0.9455 - acc: 0.8600\n",
      "Epoch 5/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.9164 - acc: 0.8609\n",
      "Epoch 5: loss improved from 0.94546 to 0.91667, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 11s 61ms/step - loss: 0.9167 - acc: 0.8609\n",
      "Epoch 6/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.8895 - acc: 0.8626\n",
      "Epoch 6: loss improved from 0.91667 to 0.88977, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 62ms/step - loss: 0.8898 - acc: 0.8626\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.8640\n",
      "Epoch 7: loss improved from 0.88977 to 0.86542, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.8654 - acc: 0.8641\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.8386 - acc: 0.8664\n",
      "Epoch 8: loss improved from 0.86542 to 0.83967, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 63ms/step - loss: 0.8397 - acc: 0.8662\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.8131 - acc: 0.8679\n",
      "Epoch 9: loss improved from 0.83967 to 0.81311, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 12s 64ms/step - loss: 0.8131 - acc: 0.8679\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7849 - acc: 0.8705\n",
      "Epoch 10: loss improved from 0.81311 to 0.78489, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 70ms/step - loss: 0.7849 - acc: 0.8705\n",
      "Q: 무서워서 달려왔어\n",
      "A: 잘 가 해보세요 \n",
      "\n",
      "\n",
      "Q: 불면증 온거 같아\n",
      "A: 잘 가 해보세요 \n",
      "\n",
      "\n",
      "Q: 답 이 어디 있을까\n",
      "A: 잘 가 될거예요 \n",
      "\n",
      "\n",
      "processing epoch: 11...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7543 - acc: 0.8730\n",
      "Epoch 1: loss improved from 0.78489 to 0.75432, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.7543 - acc: 0.8730\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.7252 - acc: 0.8759\n",
      "Epoch 2: loss improved from 0.75432 to 0.72518, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.7252 - acc: 0.8759\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6959 - acc: 0.8793\n",
      "Epoch 3: loss improved from 0.72518 to 0.69589, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 20s 106ms/step - loss: 0.6959 - acc: 0.8793\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6657 - acc: 0.8828\n",
      "Epoch 4: loss improved from 0.69589 to 0.66569, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 103ms/step - loss: 0.6657 - acc: 0.8828\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6372 - acc: 0.8858\n",
      "Epoch 5: loss improved from 0.66569 to 0.63725, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 0.6372 - acc: 0.8858\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.6056 - acc: 0.8904\n",
      "Epoch 6: loss improved from 0.63725 to 0.60562, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.6056 - acc: 0.8904\n",
      "Epoch 7/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.5749 - acc: 0.8954\n",
      "Epoch 7: loss improved from 0.60562 to 0.57495, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.5749 - acc: 0.8954\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.9017\n",
      "Epoch 8: loss improved from 0.57495 to 0.54306, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 68ms/step - loss: 0.5431 - acc: 0.9017\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.5131 - acc: 0.9077\n",
      "Epoch 9: loss improved from 0.54306 to 0.51314, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.5131 - acc: 0.9076\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.4850 - acc: 0.9125\n",
      "Epoch 10: loss improved from 0.51314 to 0.48500, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.4850 - acc: 0.9125\n",
      "Q: 교양 수업 재밌어\n",
      "A: 잘 될거예요 \n",
      "\n",
      "\n",
      "Q: 건강 관리\n",
      "A: 잘 되길 바랍니다 \n",
      "\n",
      "\n",
      "Q: 사람 만나는거 너무 어려워\n",
      "A: 잘 될거예요 \n",
      "\n",
      "\n",
      "processing epoch: 21...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4603 - acc: 0.9179\n",
      "Epoch 1: loss improved from 0.48500 to 0.46033, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 77ms/step - loss: 0.4603 - acc: 0.9179\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4331 - acc: 0.9231\n",
      "Epoch 2: loss improved from 0.46033 to 0.43308, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.4331 - acc: 0.9231\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.4086 - acc: 0.9273\n",
      "Epoch 3: loss improved from 0.43308 to 0.40856, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.4086 - acc: 0.9273\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3867 - acc: 0.9316\n",
      "Epoch 4: loss improved from 0.40856 to 0.38668, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 91ms/step - loss: 0.3867 - acc: 0.9316\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3661 - acc: 0.9355\n",
      "Epoch 5: loss improved from 0.38668 to 0.36607, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 103ms/step - loss: 0.3661 - acc: 0.9355\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3478 - acc: 0.9387\n",
      "Epoch 6: loss improved from 0.36607 to 0.34781, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 118ms/step - loss: 0.3478 - acc: 0.9387\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.3313 - acc: 0.9419\n",
      "Epoch 7: loss improved from 0.34781 to 0.33135, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 0.3313 - acc: 0.9419\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.9443\n",
      "Epoch 8: loss improved from 0.33135 to 0.31638, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.3164 - acc: 0.9444\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - ETA: 0s - loss: 0.3026 - acc: 0.9465\n",
      "Epoch 9: loss improved from 0.31638 to 0.30261, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.3026 - acc: 0.9465\n",
      "Epoch 10/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.2901 - acc: 0.9488\n",
      "Epoch 10: loss improved from 0.30261 to 0.29002, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 69ms/step - loss: 0.2900 - acc: 0.9488\n",
      "Q: 병원 너무 가기 싫다\n",
      "A: 맛있게 드세요 \n",
      "\n",
      "\n",
      "Q: 나 는 왜 이렇게 태어났을까\n",
      "A: 저 도 궁금하네요 \n",
      "\n",
      "\n",
      "Q: 나 교직이수 할수있을까\n",
      "A: 제 가 있잖아요 \n",
      "\n",
      "\n",
      "processing epoch: 31...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2795 - acc: 0.9504\n",
      "Epoch 1: loss improved from 0.29002 to 0.27953, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.2795 - acc: 0.9504\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2690 - acc: 0.9518\n",
      "Epoch 2: loss improved from 0.27953 to 0.26903, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.2690 - acc: 0.9518\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2600 - acc: 0.9532\n",
      "Epoch 3: loss improved from 0.26903 to 0.25996, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.2600 - acc: 0.9532\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2519 - acc: 0.9544\n",
      "Epoch 4: loss improved from 0.25996 to 0.25190, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 92ms/step - loss: 0.2519 - acc: 0.9544\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2440 - acc: 0.9560\n",
      "Epoch 5: loss improved from 0.25190 to 0.24403, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 0.2440 - acc: 0.9560\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2363 - acc: 0.9565\n",
      "Epoch 6: loss improved from 0.24403 to 0.23633, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 115ms/step - loss: 0.2363 - acc: 0.9565\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2307 - acc: 0.9573\n",
      "Epoch 7: loss improved from 0.23633 to 0.23075, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 26s 140ms/step - loss: 0.2307 - acc: 0.9573\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2238 - acc: 0.9583\n",
      "Epoch 8: loss improved from 0.23075 to 0.22375, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.2238 - acc: 0.9583\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2181 - acc: 0.9588\n",
      "Epoch 9: loss improved from 0.22375 to 0.21809, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.2181 - acc: 0.9588\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2118 - acc: 0.9599\n",
      "Epoch 10: loss improved from 0.21809 to 0.21181, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.2118 - acc: 0.9599\n",
      "Q: 아빠 술좀 끊으라고 해줘\n",
      "A: 자신 을 사랑 할수록 외부 의 인정 은 필요없어요 \n",
      "\n",
      "\n",
      "Q: 길 에서 헌팅 당했어\n",
      "A: 친구 를 사귈수있을거예요 \n",
      "\n",
      "\n",
      "Q: 드디어 졸업 한다\n",
      "A: 공부 하러 왔나 봐요 \n",
      "\n",
      "\n",
      "processing epoch: 41...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2059 - acc: 0.9604\n",
      "Epoch 1: loss improved from 0.21181 to 0.20589, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 13s 72ms/step - loss: 0.2059 - acc: 0.9604\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.2015 - acc: 0.9612\n",
      "Epoch 2: loss improved from 0.20589 to 0.20149, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.2015 - acc: 0.9612\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1968 - acc: 0.9615\n",
      "Epoch 3: loss improved from 0.20149 to 0.19676, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 0.1968 - acc: 0.9615\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1918 - acc: 0.9619\n",
      "Epoch 4: loss improved from 0.19676 to 0.19182, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 104ms/step - loss: 0.1918 - acc: 0.9619\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1880 - acc: 0.9624\n",
      "Epoch 5: loss improved from 0.19182 to 0.18802, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 109ms/step - loss: 0.1880 - acc: 0.9624\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1841 - acc: 0.9626\n",
      "Epoch 6: loss improved from 0.18802 to 0.18408, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 113ms/step - loss: 0.1841 - acc: 0.9626\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1813 - acc: 0.9631\n",
      "Epoch 7: loss improved from 0.18408 to 0.18133, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 112ms/step - loss: 0.1813 - acc: 0.9631\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1786 - acc: 0.9629\n",
      "Epoch 8: loss improved from 0.18133 to 0.17859, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 20s 108ms/step - loss: 0.1786 - acc: 0.9629\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1734 - acc: 0.9636\n",
      "Epoch 9: loss improved from 0.17859 to 0.17336, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.1734 - acc: 0.9636\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1704 - acc: 0.9642\n",
      "Epoch 10: loss improved from 0.17336 to 0.17040, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1704 - acc: 0.9642\n",
      "Q: 뭐 할줄 알 아\n",
      "A: 시원한 물이 라도 한잔 드세요 \n",
      "\n",
      "\n",
      "Q: 신혼여행 한 달 쯤 가고싶다\n",
      "A: 잘 해 결 되길 바라요 \n",
      "\n",
      "\n",
      "Q: 밥 태웠어\n",
      "A: 맛있게 드세요 \n",
      "\n",
      "\n",
      "processing epoch: 51...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1676 - acc: 0.9644\n",
      "Epoch 1: loss improved from 0.17040 to 0.16758, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 86ms/step - loss: 0.1676 - acc: 0.9644\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1644 - acc: 0.9648\n",
      "Epoch 2: loss improved from 0.16758 to 0.16445, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 94ms/step - loss: 0.1644 - acc: 0.9648\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1635 - acc: 0.9645\n",
      "Epoch 3: loss improved from 0.16445 to 0.16348, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 89ms/step - loss: 0.1635 - acc: 0.9645\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1602 - acc: 0.9649\n",
      "Epoch 4: loss improved from 0.16348 to 0.16021, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 0.1602 - acc: 0.9649\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1565 - acc: 0.9655\n",
      "Epoch 5: loss improved from 0.16021 to 0.15651, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 24s 126ms/step - loss: 0.1565 - acc: 0.9655\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1557 - acc: 0.9653\n",
      "Epoch 6: loss improved from 0.15651 to 0.15572, saving model to model\\chatmodel-attention-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 23s 123ms/step - loss: 0.1557 - acc: 0.9653\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1536 - acc: 0.9657\n",
      "Epoch 7: loss improved from 0.15572 to 0.15363, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 90ms/step - loss: 0.1536 - acc: 0.9657\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9655\n",
      "Epoch 8: loss improved from 0.15363 to 0.15019, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.1502 - acc: 0.9655\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1472 - acc: 0.9660\n",
      "Epoch 9: loss improved from 0.15019 to 0.14725, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 72ms/step - loss: 0.1472 - acc: 0.9660\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1431 - acc: 0.9667\n",
      "Epoch 10: loss improved from 0.14725 to 0.14313, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1431 - acc: 0.9667\n",
      "Q: 남자친구 가 사업 한 대\n",
      "A: 다른 생각 을 해보세요 \n",
      "\n",
      "\n",
      "Q: 무리 에 잘 못 낀듯\n",
      "A: 공부 하면 더 많은 선택 을 할수있죠 \n",
      "\n",
      "\n",
      "Q: 내기 해서 이겼는데 소원 뭐 하 지\n",
      "A: 이야기 를 하지 않고 결정 했나 봐요 \n",
      "\n",
      "\n",
      "processing epoch: 61...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1395 - acc: 0.9670\n",
      "Epoch 1: loss improved from 0.14313 to 0.13952, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 79ms/step - loss: 0.1395 - acc: 0.9670\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1383 - acc: 0.9672\n",
      "Epoch 2: loss improved from 0.13952 to 0.13826, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 93ms/step - loss: 0.1383 - acc: 0.9672\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1341 - acc: 0.9677\n",
      "Epoch 3: loss improved from 0.13826 to 0.13411, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 88ms/step - loss: 0.1341 - acc: 0.9677\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1365 - acc: 0.9674\n",
      "Epoch 4: loss did not improve from 0.13411\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 0.1365 - acc: 0.9674\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1317 - acc: 0.9684\n",
      "Epoch 5: loss improved from 0.13411 to 0.13172, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 113ms/step - loss: 0.1317 - acc: 0.9684\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1302 - acc: 0.9683\n",
      "Epoch 6: loss improved from 0.13172 to 0.13019, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 109ms/step - loss: 0.1302 - acc: 0.9683\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1252 - acc: 0.9690\n",
      "Epoch 7: loss improved from 0.13019 to 0.12522, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 86ms/step - loss: 0.1252 - acc: 0.9690\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1227 - acc: 0.9694\n",
      "Epoch 8: loss improved from 0.12522 to 0.12266, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1227 - acc: 0.9694\n",
      "Epoch 9/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1202 - acc: 0.9699\n",
      "Epoch 9: loss improved from 0.12266 to 0.12016, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.1202 - acc: 0.9699\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1198 - acc: 0.9698\n",
      "Epoch 10: loss improved from 0.12016 to 0.11979, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 74ms/step - loss: 0.1198 - acc: 0.9698\n",
      "Q: 시계 소리 가 거슬려\n",
      "A: 아무래도 덜 경쟁 하겠죠 \n",
      "\n",
      "\n",
      "Q: 관절염 인가\n",
      "A: 잘 알아보고 사세요 \n",
      "\n",
      "\n",
      "Q: 기본 이 안되어있어\n",
      "A: 각자 가 생각 하 는 기본 이 다를수도 있어요 \n",
      "\n",
      "\n",
      "processing epoch: 71...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1174 - acc: 0.9704\n",
      "Epoch 1: loss improved from 0.11979 to 0.11737, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.1174 - acc: 0.9704\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1145 - acc: 0.9710\n",
      "Epoch 2: loss improved from 0.11737 to 0.11447, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.1145 - acc: 0.9710\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1110 - acc: 0.9711\n",
      "Epoch 3: loss improved from 0.11447 to 0.11097, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 100ms/step - loss: 0.1110 - acc: 0.9711\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1082 - acc: 0.9721\n",
      "Epoch 4: loss improved from 0.11097 to 0.10819, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 0.1082 - acc: 0.9721\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1062 - acc: 0.9721\n",
      "Epoch 5: loss improved from 0.10819 to 0.10620, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 24s 127ms/step - loss: 0.1062 - acc: 0.9721\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1037 - acc: 0.9727\n",
      "Epoch 6: loss improved from 0.10620 to 0.10374, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 0.1037 - acc: 0.9727\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.1031 - acc: 0.9727\n",
      "Epoch 7: loss improved from 0.10374 to 0.10311, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.1031 - acc: 0.9727\n",
      "Epoch 8/10\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.1020 - acc: 0.9732\n",
      "Epoch 8: loss improved from 0.10311 to 0.10199, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.1020 - acc: 0.9732\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0984 - acc: 0.9737\n",
      "Epoch 9: loss improved from 0.10199 to 0.09841, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.0984 - acc: 0.9737\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0959 - acc: 0.9743\n",
      "Epoch 10: loss improved from 0.09841 to 0.09588, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0959 - acc: 0.9743\n",
      "Q: 빚 때문 에 죽을것 같아\n",
      "A: 빚 얼른 갚고 새 출발 하세요 \n",
      "\n",
      "\n",
      "Q: 누 가문 열다 내 차 긁었어\n",
      "A: 보험 처리 하세요 \n",
      "\n",
      "\n",
      "Q: 내 가 너무 쉽게 보였나\n",
      "A: 같이 여행 을 떠나 보세요 \n",
      "\n",
      "\n",
      "processing epoch: 81...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0952 - acc: 0.9745\n",
      "Epoch 1: loss improved from 0.09588 to 0.09519, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 89ms/step - loss: 0.0952 - acc: 0.9745\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0939 - acc: 0.9750\n",
      "Epoch 2: loss improved from 0.09519 to 0.09389, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 0.0939 - acc: 0.9750\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0926 - acc: 0.9746\n",
      "Epoch 3: loss improved from 0.09389 to 0.09256, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 119ms/step - loss: 0.0926 - acc: 0.9746\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0884 - acc: 0.9760\n",
      "Epoch 4: loss improved from 0.09256 to 0.08836, saving model to model\\chatmodel-attention-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 23s 121ms/step - loss: 0.0884 - acc: 0.9760\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0857 - acc: 0.9762\n",
      "Epoch 5: loss improved from 0.08836 to 0.08566, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 94ms/step - loss: 0.0857 - acc: 0.9762\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0841 - acc: 0.9768\n",
      "Epoch 6: loss improved from 0.08566 to 0.08409, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0841 - acc: 0.9768\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0816 - acc: 0.9776\n",
      "Epoch 7: loss improved from 0.08409 to 0.08160, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0816 - acc: 0.9776\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0792 - acc: 0.9780\n",
      "Epoch 8: loss improved from 0.08160 to 0.07919, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0792 - acc: 0.9780\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0765 - acc: 0.9787\n",
      "Epoch 9: loss improved from 0.07919 to 0.07645, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 87ms/step - loss: 0.0765 - acc: 0.9787\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0744 - acc: 0.9790\n",
      "Epoch 10: loss improved from 0.07645 to 0.07441, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 101ms/step - loss: 0.0744 - acc: 0.9790\n",
      "Q: 숙제 안 한듯\n",
      "A: 미리 충전 하세요 \n",
      "\n",
      "\n",
      "Q: 맞는 결정 을 한거겠지\n",
      "A: 네 이제 잘 해낼 차례 예요 \n",
      "\n",
      "\n",
      "Q: 괜히 아까운 시간 버렸다\n",
      "A: 그것 도 다 경험 이라고 생각 하세요 \n",
      "\n",
      "\n",
      "processing epoch: 91...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0741 - acc: 0.9792\n",
      "Epoch 1: loss improved from 0.07441 to 0.07407, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 118ms/step - loss: 0.0741 - acc: 0.9792\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0721 - acc: 0.9798\n",
      "Epoch 2: loss improved from 0.07407 to 0.07210, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 116ms/step - loss: 0.0721 - acc: 0.9798\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0781 - acc: 0.9789\n",
      "Epoch 3: loss did not improve from 0.07210\n",
      "188/188 [==============================] - 17s 88ms/step - loss: 0.0781 - acc: 0.9789\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0697 - acc: 0.9808\n",
      "Epoch 4: loss improved from 0.07210 to 0.06971, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 76ms/step - loss: 0.0697 - acc: 0.9808\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0667 - acc: 0.9814\n",
      "Epoch 5: loss improved from 0.06971 to 0.06671, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.0667 - acc: 0.9814\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0668 - acc: 0.9814\n",
      "Epoch 6: loss did not improve from 0.06671\n",
      "188/188 [==============================] - 15s 79ms/step - loss: 0.0668 - acc: 0.9814\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0623 - acc: 0.9823\n",
      "Epoch 7: loss improved from 0.06671 to 0.06227, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.0623 - acc: 0.9823\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0601 - acc: 0.9831\n",
      "Epoch 8: loss improved from 0.06227 to 0.06006, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 88ms/step - loss: 0.0601 - acc: 0.9831\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0601 - acc: 0.9834\n",
      "Epoch 9: loss did not improve from 0.06006\n",
      "188/188 [==============================] - 20s 104ms/step - loss: 0.0601 - acc: 0.9834\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0571 - acc: 0.9839\n",
      "Epoch 10: loss improved from 0.06006 to 0.05713, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 119ms/step - loss: 0.0571 - acc: 0.9839\n",
      "Q: 나짤릴거같 아\n",
      "A: 축하 합니다 \n",
      "\n",
      "\n",
      "Q: 공부 로 먹고살수있을까\n",
      "A: 지금 처럼 잘 될거예요 \n",
      "\n",
      "\n",
      "Q: 발렌타인데이 초콜릿 비싸\n",
      "A: 초콜릿 녹 여서 예쁘게만 드는것도 재밌죠 \n",
      "\n",
      "\n",
      "processing epoch: 101...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0570 - acc: 0.9838\n",
      "Epoch 1: loss improved from 0.05713 to 0.05698, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 110ms/step - loss: 0.0570 - acc: 0.9838\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0541 - acc: 0.9851\n",
      "Epoch 2: loss improved from 0.05698 to 0.05409, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0541 - acc: 0.9851\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0600 - acc: 0.9836\n",
      "Epoch 3: loss did not improve from 0.05409\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0600 - acc: 0.9836\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0584 - acc: 0.9838\n",
      "Epoch 4: loss did not improve from 0.05409\n",
      "188/188 [==============================] - 14s 73ms/step - loss: 0.0584 - acc: 0.9838\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0554 - acc: 0.9842\n",
      "Epoch 5: loss did not improve from 0.05409\n",
      "188/188 [==============================] - 16s 86ms/step - loss: 0.0554 - acc: 0.9842\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0511 - acc: 0.9858\n",
      "Epoch 6: loss improved from 0.05409 to 0.05109, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 87ms/step - loss: 0.0511 - acc: 0.9858\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0487 - acc: 0.9865\n",
      "Epoch 7: loss improved from 0.05109 to 0.04873, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 92ms/step - loss: 0.0487 - acc: 0.9865\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0477 - acc: 0.9867\n",
      "Epoch 8: loss improved from 0.04873 to 0.04770, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 20s 108ms/step - loss: 0.0477 - acc: 0.9867\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0456 - acc: 0.9871\n",
      "Epoch 9: loss improved from 0.04770 to 0.04559, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 24s 125ms/step - loss: 0.0456 - acc: 0.9871\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0433 - acc: 0.9879\n",
      "Epoch 10: loss improved from 0.04559 to 0.04329, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 20s 108ms/step - loss: 0.0433 - acc: 0.9879\n",
      "Q: 배 에서 소리 남\n",
      "A: 다음 학기 를 노려보세요 \n",
      "\n",
      "\n",
      "Q: 나 문제 가 많은거 같아\n",
      "A: 문제 는 해결 하 라고 있는거죠 \n",
      "\n",
      "\n",
      "Q: 면접 잘 볼수있을까\n",
      "A: 회사 와 자신 에 대해 서더 공부 해서 자신감 을 가져 보세요 \n",
      "\n",
      "\n",
      "processing epoch: 111...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0425 - acc: 0.9883\n",
      "Epoch 1: loss improved from 0.04329 to 0.04250, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 79ms/step - loss: 0.0425 - acc: 0.9883\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0413 - acc: 0.9886\n",
      "Epoch 2: loss improved from 0.04250 to 0.04134, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.0413 - acc: 0.9886\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0408 - acc: 0.9890\n",
      "Epoch 3: loss improved from 0.04134 to 0.04085, saving model to model\\chatmodel-attention-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 17s 90ms/step - loss: 0.0408 - acc: 0.9890\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0393 - acc: 0.9893\n",
      "Epoch 4: loss improved from 0.04085 to 0.03927, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 101ms/step - loss: 0.0393 - acc: 0.9893\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0383 - acc: 0.9894\n",
      "Epoch 5: loss improved from 0.03927 to 0.03830, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 23s 125ms/step - loss: 0.0383 - acc: 0.9894\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0371 - acc: 0.9897\n",
      "Epoch 6: loss improved from 0.03830 to 0.03708, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 0.0371 - acc: 0.9897\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0382 - acc: 0.9895\n",
      "Epoch 7: loss did not improve from 0.03708\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 0.0382 - acc: 0.9895\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0371 - acc: 0.9899\n",
      "Epoch 8: loss did not improve from 0.03708\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.0371 - acc: 0.9899\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0342 - acc: 0.9905\n",
      "Epoch 9: loss improved from 0.03708 to 0.03423, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 14s 75ms/step - loss: 0.0342 - acc: 0.9905\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0341 - acc: 0.9907\n",
      "Epoch 10: loss improved from 0.03423 to 0.03408, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0341 - acc: 0.9907\n",
      "Q: 공복 이라 예민해\n",
      "A: 자연스러운 현상 이에요 \n",
      "\n",
      "\n",
      "Q: 발표 가 안나\n",
      "A: 행운 을 빌어 요 \n",
      "\n",
      "\n",
      "Q: 스노우보드 배우러 갈거 댜\n",
      "A: 저 도배 워 보고싶어요 \n",
      "\n",
      "\n",
      "processing epoch: 121...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0325 - acc: 0.9912\n",
      "Epoch 1: loss improved from 0.03408 to 0.03253, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 90ms/step - loss: 0.0325 - acc: 0.9912\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0298 - acc: 0.9923\n",
      "Epoch 2: loss improved from 0.03253 to 0.02982, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 19s 101ms/step - loss: 0.0298 - acc: 0.9923\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0308 - acc: 0.9917\n",
      "Epoch 3: loss did not improve from 0.02982\n",
      "188/188 [==============================] - 20s 108ms/step - loss: 0.0308 - acc: 0.9917\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0346 - acc: 0.9907\n",
      "Epoch 4: loss did not improve from 0.02982\n",
      "188/188 [==============================] - 23s 120ms/step - loss: 0.0346 - acc: 0.9907\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0307 - acc: 0.9918\n",
      "Epoch 5: loss did not improve from 0.02982\n",
      "188/188 [==============================] - 21s 110ms/step - loss: 0.0307 - acc: 0.9918\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0323 - acc: 0.9914\n",
      "Epoch 6: loss did not improve from 0.02982\n",
      "188/188 [==============================] - 16s 86ms/step - loss: 0.0323 - acc: 0.9914\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0287 - acc: 0.9922\n",
      "Epoch 7: loss improved from 0.02982 to 0.02866, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0287 - acc: 0.9922\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0307 - acc: 0.9915\n",
      "Epoch 8: loss did not improve from 0.02866\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0307 - acc: 0.9915\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0277 - acc: 0.9926\n",
      "Epoch 9: loss improved from 0.02866 to 0.02765, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 93ms/step - loss: 0.0277 - acc: 0.9926\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0251 - acc: 0.9931\n",
      "Epoch 10: loss improved from 0.02765 to 0.02509, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 20s 105ms/step - loss: 0.0251 - acc: 0.9931\n",
      "Q: 뭐라고하고 싶었는데 참았어\n",
      "A: 계속 그러면말하세요 \n",
      "\n",
      "\n",
      "Q: 내 남자친구 보고싶어\n",
      "A: 네 알려주세요 \n",
      "\n",
      "\n",
      "Q: 가스 비 너무 많이 나왔다\n",
      "A: 다음 달 에는 더 절약 해봐요 \n",
      "\n",
      "\n",
      "processing epoch: 131...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0233 - acc: 0.9938\n",
      "Epoch 1: loss improved from 0.02509 to 0.02331, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 24s 127ms/step - loss: 0.0233 - acc: 0.9938\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0235 - acc: 0.9939\n",
      "Epoch 2: loss did not improve from 0.02331\n",
      "188/188 [==============================] - 19s 103ms/step - loss: 0.0235 - acc: 0.9939\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0219 - acc: 0.9941\n",
      "Epoch 3: loss improved from 0.02331 to 0.02191, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.0219 - acc: 0.9941\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0205 - acc: 0.9946\n",
      "Epoch 4: loss improved from 0.02191 to 0.02050, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 83ms/step - loss: 0.0205 - acc: 0.9946\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0213 - acc: 0.9943\n",
      "Epoch 5: loss did not improve from 0.02050\n",
      "188/188 [==============================] - 17s 90ms/step - loss: 0.0213 - acc: 0.9943\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0219 - acc: 0.9942\n",
      "Epoch 6: loss did not improve from 0.02050\n",
      "188/188 [==============================] - 19s 102ms/step - loss: 0.0219 - acc: 0.9942\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0213 - acc: 0.9942\n",
      "Epoch 7: loss did not improve from 0.02050\n",
      "188/188 [==============================] - 23s 121ms/step - loss: 0.0213 - acc: 0.9942\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9949\n",
      "Epoch 8: loss improved from 0.02050 to 0.01992, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 20s 104ms/step - loss: 0.0199 - acc: 0.9949\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9946\n",
      "Epoch 9: loss improved from 0.01992 to 0.01990, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0199 - acc: 0.9946\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0199 - acc: 0.9948\n",
      "Epoch 10: loss did not improve from 0.01990\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.0199 - acc: 0.9948\n",
      "Q: 먹고나니까 졸리 당\n",
      "A: 자연 의 이치 죠 안녕히주 무세요 \n",
      "\n",
      "\n",
      "Q: 내 가 무능력하게 느껴져\n",
      "A: 잘 할수있는게 다른 거 예요 \n",
      "\n",
      "\n",
      "Q: 스키장 알바 재밌대\n",
      "A: 다음 에는 받을수있을거예요 \n",
      "\n",
      "\n",
      "processing epoch: 141...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0183 - acc: 0.9954\n",
      "Epoch 1: loss improved from 0.01990 to 0.01831, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 91ms/step - loss: 0.0183 - acc: 0.9954\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0194 - acc: 0.9948\n",
      "Epoch 2: loss did not improve from 0.01831\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 0.0194 - acc: 0.9948\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9954\n",
      "Epoch 3: loss improved from 0.01831 to 0.01746, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 111ms/step - loss: 0.0175 - acc: 0.9954\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - ETA: 0s - loss: 0.0182 - acc: 0.9953\n",
      "Epoch 4: loss did not improve from 0.01746\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 0.0182 - acc: 0.9953\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.9957\n",
      "Epoch 5: loss did not improve from 0.01746\n",
      "188/188 [==============================] - 20s 105ms/step - loss: 0.0175 - acc: 0.9957\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0157 - acc: 0.9958\n",
      "Epoch 6: loss improved from 0.01746 to 0.01571, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0157 - acc: 0.9958\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 7: loss improved from 0.01571 to 0.01521, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 82ms/step - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0143 - acc: 0.9962\n",
      "Epoch 8: loss improved from 0.01521 to 0.01433, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 87ms/step - loss: 0.0143 - acc: 0.9962\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0147 - acc: 0.9961\n",
      "Epoch 9: loss did not improve from 0.01433\n",
      "188/188 [==============================] - 19s 99ms/step - loss: 0.0147 - acc: 0.9961\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9963\n",
      "Epoch 10: loss improved from 0.01433 to 0.01399, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 21s 111ms/step - loss: 0.0140 - acc: 0.9963\n",
      "Q: 갈까말까 고민 돼\n",
      "A: 가세 요 \n",
      "\n",
      "\n",
      "Q: 기차 타고 여행가 고 싶어\n",
      "A: 꿈꾸던 여행 이네 요 \n",
      "\n",
      "\n",
      "Q: 남편 이 회식 이라 고안 와\n",
      "A: 사회생활 을 이해 해주세요 \n",
      "\n",
      "\n",
      "processing epoch: 151...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0141 - acc: 0.9962\n",
      "Epoch 1: loss did not improve from 0.01399\n",
      "188/188 [==============================] - 22s 120ms/step - loss: 0.0141 - acc: 0.9962\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0129 - acc: 0.9968\n",
      "Epoch 2: loss improved from 0.01399 to 0.01292, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 0.0129 - acc: 0.9968\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0137 - acc: 0.9967\n",
      "Epoch 3: loss did not improve from 0.01292\n",
      "188/188 [==============================] - 15s 79ms/step - loss: 0.0137 - acc: 0.9967\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.9962\n",
      "Epoch 4: loss did not improve from 0.01292\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.0140 - acc: 0.9962\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0137 - acc: 0.9964\n",
      "Epoch 5: loss did not improve from 0.01292\n",
      "188/188 [==============================] - 17s 92ms/step - loss: 0.0137 - acc: 0.9964\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0119 - acc: 0.9970\n",
      "Epoch 6: loss improved from 0.01292 to 0.01193, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 97ms/step - loss: 0.0119 - acc: 0.9970\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0107 - acc: 0.9974\n",
      "Epoch 7: loss improved from 0.01193 to 0.01072, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 116ms/step - loss: 0.0107 - acc: 0.9974\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0124 - acc: 0.9969\n",
      "Epoch 8: loss did not improve from 0.01072\n",
      "188/188 [==============================] - 26s 136ms/step - loss: 0.0124 - acc: 0.9969\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0127 - acc: 0.9967\n",
      "Epoch 9: loss did not improve from 0.01072\n",
      "188/188 [==============================] - 19s 103ms/step - loss: 0.0127 - acc: 0.9967\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0125 - acc: 0.9968\n",
      "Epoch 10: loss did not improve from 0.01072\n",
      "188/188 [==============================] - 17s 89ms/step - loss: 0.0125 - acc: 0.9968\n",
      "Q: 너무 멋있다\n",
      "A: 제 가 생각 해도 저 는 너무 멋있는거 같아요 \n",
      "\n",
      "\n",
      "Q: 시간 진짜 빨리 감\n",
      "A: 시간 은 상대 적 으로 흘러갑니다 \n",
      "\n",
      "\n",
      "Q: 그만두고 나오고싶어\n",
      "A: 뒷 감당 자신있으면하세요 \n",
      "\n",
      "\n",
      "processing epoch: 161...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0115 - acc: 0.9971\n",
      "Epoch 1: loss did not improve from 0.01072\n",
      "188/188 [==============================] - 15s 80ms/step - loss: 0.0115 - acc: 0.9971\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0115 - acc: 0.9969\n",
      "Epoch 2: loss did not improve from 0.01072\n",
      "188/188 [==============================] - 16s 87ms/step - loss: 0.0115 - acc: 0.9969\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.9974\n",
      "Epoch 3: loss improved from 0.01072 to 0.01018, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 0.0102 - acc: 0.9974\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9978\n",
      "Epoch 4: loss improved from 0.01018 to 0.00924, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 22s 116ms/step - loss: 0.0092 - acc: 0.9978\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9977\n",
      "Epoch 5: loss improved from 0.00924 to 0.00910, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 23s 121ms/step - loss: 0.0091 - acc: 0.9977\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9980\n",
      "Epoch 6: loss improved from 0.00910 to 0.00817, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 93ms/step - loss: 0.0082 - acc: 0.9980\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 7: loss improved from 0.00817 to 0.00797, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0080 - acc: 0.9980\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 8: loss did not improve from 0.00797\n",
      "188/188 [==============================] - 15s 81ms/step - loss: 0.0101 - acc: 0.9974\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0125 - acc: 0.9971\n",
      "Epoch 9: loss did not improve from 0.00797\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.0125 - acc: 0.9971\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0092 - acc: 0.9977\n",
      "Epoch 10: loss did not improve from 0.00797\n",
      "188/188 [==============================] - 18s 96ms/step - loss: 0.0092 - acc: 0.9977\n",
      "Q: 배고파 서자 다 깼어\n",
      "A: 맛 난 거드세요 \n",
      "\n",
      "\n",
      "Q: 세뱃돈 받았어 호호\n",
      "A: 아직 어린 가봐요 \n",
      "\n",
      "\n",
      "Q: 성공할수있을까\n",
      "A: 지금 보다 더 잘살거예요 \n",
      "\n",
      "\n",
      "processing epoch: 171...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 1: loss did not improve from 0.00797\n",
      "188/188 [==============================] - 22s 115ms/step - loss: 0.0087 - acc: 0.9977\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9980\n",
      "Epoch 2: loss improved from 0.00797 to 0.00790, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 25s 134ms/step - loss: 0.0079 - acc: 0.9980\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 3: loss did not improve from 0.00790\n",
      "188/188 [==============================] - 20s 106ms/step - loss: 0.0083 - acc: 0.9979\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9981\n",
      "Epoch 4: loss improved from 0.00790 to 0.00787, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.0079 - acc: 0.9981\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 5: loss improved from 0.00787 to 0.00644, saving model to model\\chatmodel-attention-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0064 - acc: 0.9985\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.9987\n",
      "Epoch 6: loss improved from 0.00644 to 0.00581, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 90ms/step - loss: 0.0058 - acc: 0.9987\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 7: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 19s 103ms/step - loss: 0.0063 - acc: 0.9985\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 8: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 23s 125ms/step - loss: 0.0069 - acc: 0.9983\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 9: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 22s 116ms/step - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9980\n",
      "Epoch 10: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 17s 92ms/step - loss: 0.0076 - acc: 0.9980\n",
      "Q: 법 을 피해 가는 사람\n",
      "A: 능력 이긴하지만 꼬리 가길면 밟 힐거 예요 \n",
      "\n",
      "\n",
      "Q: 내 일이 기대 돼\n",
      "A: 좋은 일이 생길거예요 \n",
      "\n",
      "\n",
      "Q: 꽃다발 선물 괜찮지\n",
      "A: 센스 있는 선물 이에요 \n",
      "\n",
      "\n",
      "processing epoch: 181...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 1: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0080 - acc: 0.9979\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 2: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 17s 91ms/step - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 3: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 18s 98ms/step - loss: 0.0071 - acc: 0.9983\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9980\n",
      "Epoch 4: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 23s 123ms/step - loss: 0.0079 - acc: 0.9980\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9981\n",
      "Epoch 5: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 23s 122ms/step - loss: 0.0079 - acc: 0.9981\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 6: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 19s 103ms/step - loss: 0.0064 - acc: 0.9984\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9984\n",
      "Epoch 7: loss did not improve from 0.00581\n",
      "188/188 [==============================] - 17s 91ms/step - loss: 0.0063 - acc: 0.9984\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 8: loss improved from 0.00581 to 0.00570, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 17s 89ms/step - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 9: loss did not improve from 0.00570\n",
      "188/188 [==============================] - 18s 94ms/step - loss: 0.0067 - acc: 0.9983\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9987\n",
      "Epoch 10: loss did not improve from 0.00570\n",
      "188/188 [==============================] - 25s 132ms/step - loss: 0.0057 - acc: 0.9987\n",
      "Q: 개강 룩 입어볼까\n",
      "A: 개시 해보세요 \n",
      "\n",
      "\n",
      "Q: 동상 걸릴 뻔했어\n",
      "A: 감기 조심하세요 \n",
      "\n",
      "\n",
      "Q: 시험 기간 인데 집중 이안 돼\n",
      "A: 딴 생각 하 지 마세요 \n",
      "\n",
      "\n",
      "processing epoch: 191...\n",
      "Epoch 1/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 1: loss improved from 0.00570 to 0.00504, saving model to model\\chatmodel-attention-checkpoint.ckpt\n",
      "188/188 [==============================] - 25s 135ms/step - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 2/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0060 - acc: 0.9984\n",
      "Epoch 2: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 20s 107ms/step - loss: 0.0060 - acc: 0.9984\n",
      "Epoch 3/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0073 - acc: 0.9981\n",
      "Epoch 3: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 16s 84ms/step - loss: 0.0073 - acc: 0.9981\n",
      "Epoch 4/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0062 - acc: 0.9982\n",
      "Epoch 4: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 15s 79ms/step - loss: 0.0062 - acc: 0.9982\n",
      "Epoch 5/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 5: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 17s 92ms/step - loss: 0.0050 - acc: 0.9987\n",
      "Epoch 6/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 6: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 21s 113ms/step - loss: 0.0057 - acc: 0.9985\n",
      "Epoch 7/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0061 - acc: 0.9985\n",
      "Epoch 7: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 22s 118ms/step - loss: 0.0061 - acc: 0.9985\n",
      "Epoch 8/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 8: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 25s 134ms/step - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 9/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 9: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 24s 129ms/step - loss: 0.0053 - acc: 0.9986\n",
      "Epoch 10/10\n",
      "188/188 [==============================] - ETA: 0s - loss: 0.0055 - acc: 0.9986\n",
      "Epoch 10: loss did not improve from 0.00504\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.0055 - acc: 0.9986\n",
      "Q: 새 옷 샀다\n",
      "A: 꼬까 옷 개시 해보세요 \n",
      "\n",
      "\n",
      "Q: 방학 끝나서 좋아\n",
      "A: 친구 들 이 보고싶었나 봐요 \n",
      "\n",
      "\n",
      "Q: 남자친구 교회 데려가고싶어\n",
      "A: 마음 을 열 때 까지 설득 해보세요 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
    "    chat.fit([question_padded, answer_in_padded],\n",
    "                answer_out_one_hot,\n",
    "                epochs=10,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[checkpoint]\n",
    "               )\n",
    "    # 랜덤한 샘플 번호 추출\n",
    "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
    "\n",
    "    # 예측 성능 테스트\n",
    "    for idx in samples:\n",
    "        question_inputs = question_padded[idx]\n",
    "        # 문장 예측\n",
    "        results = make_prediction(chat, np.expand_dims(question_inputs, 0))\n",
    "        \n",
    "        # 변환된 인덱스를 문장으로 변환\n",
    "        results = convert_index_to_text(results, END_TOKEN)\n",
    "        \n",
    "        print(f'Q: {questions[idx]}')\n",
    "        print(f'A: {results}\\n')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a162035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c8584cb",
   "metadata": {},
   "source": [
    "## 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc8ed2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 4364,  320,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자연어 (질문 입력) 대한 전처리 함수\n",
    "def make_question(sentence):\n",
    "    sentence = clean_and_morph(sentence)\n",
    "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
    "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "    return question_padded\n",
    "\n",
    "make_question('오늘 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2e32649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문에 대한 답변을 리턴해주는 함수\n",
    "def run_chatbot(question):\n",
    "    question_inputs = make_question(question)\n",
    "    results = make_prediction(chat, question_inputs)\n",
    "    results = convert_index_to_text(results, END_TOKEN)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "448ab6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 말을 걸어 보세요!\n",
      "배고프다\n",
      ">> 챗봇 응답: 저 도밥 먹고싶어요 \n",
      "<< 말을 걸어 보세요!\n",
      "친구가 별로 없네~\n",
      ">> 챗봇 응답: 제 가 따라가려면 멀었네요 \n",
      "<< 말을 걸어 보세요!\n",
      "집에 가고 싶다.\n",
      ">> 챗봇 응답: 축하 드려요 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6768/2353151595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 콘솔에서 테스트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0muser_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<< 말을 걸어 보세요!\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muser_input\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'q'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dev\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# 콘솔에서 테스트\n",
    "while True:\n",
    "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
    "    if user_input == 'q':\n",
    "        break\n",
    "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829f773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.20.16:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [27/Apr/2022 16:58:19] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Apr/2022 16:58:19] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [27/Apr/2022 16:58:52] \"GET /chatbot?question=12 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      ">> 챗봇 응답: 독서 와 음악 감상 이라고 하고싶지만 아무 것 도안 했어요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2022 17:58:43] \"GET /chatbot?question=12 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      ">> 챗봇 응답: 독서 와 음악 감상 이라고 하고싶지만 아무 것 도안 했어요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2022 17:58:58] \"GET /chatbot?question=넌+이름이+뭐니%3F HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "넌 이름이 뭐니?\n",
      ">> 챗봇 응답: 확인 해달라고 해보세요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2022 17:59:10] \"GET /chatbot?question=알라딘이+뭐니%3F HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "알라딘이 뭐니?\n",
      ">> 챗봇 응답: 얼른실내로 들어가세요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2022 17:59:22] \"GET /chatbot?question=배고파요 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배고파요\n",
      ">> 챗봇 응답: 독서 와 음악 감상 이라고 하고싶지만 아무 것 도안 했어요 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Apr/2022 17:59:31] \"GET /chatbot?question=날씨가+좋아요~ HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날씨가 좋아요~\n",
      ">> 챗봇 응답: 시도 해봐도 좋겠죠 \n"
     ]
    }
   ],
   "source": [
    "# 웹 서비스를 위한 배포\n",
    "from flask import Flask, request\n",
    "from flask import jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "#시작 요청이 왔을 때 아래 코드를 수행\n",
    "@app.route('/', methods=['POST', 'GET'])\n",
    "def main():\n",
    "    return 'Hello Chatbot'\n",
    "\n",
    "@app.route('/chatbot', methods=['POST', 'GET'])\n",
    "def chatbot():\n",
    "    print(request.args[\"question\"])\n",
    "    answer = run_chatbot(request.args[\"question\"])\n",
    "    print('>> 챗봇 응답: {}'.format(answer))\n",
    "    response = {'answer': answer}\n",
    "    return jsonify(response)\n",
    "\n",
    "#서버 구동\n",
    "app.run('0.0.0.0', port=9000, threaded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9464915",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
